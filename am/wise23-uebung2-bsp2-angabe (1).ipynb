{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30138,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Textklassifikation\n\nIn dieser Übung werden wir das *[20newsgroups](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset)* Datenset benutzen um ein besseres Verständnis für Kernelmethoden und Klassifikation zu bekommen. Unser Ziel ist es, eine Reihe von Nachrichtenmeldungen 11 unterschiedlichen Kategorien zuzuordnen. Wir gehen dabei in zwei Schritten vor:\n- Vorbereitung der Daten mittels Feature Extraction/Vectorization\n- Klassifikation mittels Linear/Kernel SVM (Kapitel 4.2.3)","metadata":{}},{"cell_type":"code","source":"# Stelle sicher, dass das Internet (unter Settings auf der rechten Seite) für das \n# Notebook aktiviert ist, damit das Herunterladen der Daten funktioniert.\n\nfrom sklearn.datasets import fetch_20newsgroups\n\ncategories = [\n    'alt.atheism',\n    'comp.graphics',\n    'misc.forsale',\n    'rec.autos',\n    'rec.motorcycles',\n    'sci.electronics',\n    'sci.space',\n    'soc.religion.christian',\n    'talk.politics.guns',\n    'talk.politics.mideast',\n    'talk.politics.misc',\n]\n\ndata_train = fetch_20newsgroups(subset=\"train\", shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'), categories=categories)\ndata_test = fetch_20newsgroups(subset=\"test\", shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'), categories=categories)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nX_train, y_train = data_train.data, data_train.target\nX_test, y_test = data_test.data, data_test.target\nlen(X_train), len(y_train), len(np.unique(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Das Trainingsset enthält 11315 Nachrichtenmeldungen, welche 20 verschiedenen Kategorien zugewiesen sind. Diese Kategorien finden im Attribut `target_names`. Wir verwenden allerdings nur 11 dieser Kategorien, damit das Datenset nicht zu groß und die Rechenzeiten nicht zu lange werden. Das gibt uns 6200 Texte.","metadata":{}},{"cell_type":"code","source":"data_train.target_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Das Targetset `y_train` ist einfach ein *numpy array*, `X_test` ist jedoch eine Python Liste:","metadata":{}},{"cell_type":"code","source":"type(X_train), type(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wenn wir das erste Element ansehen, dann stellen wir fest, dass es sich bei jeder Instanz um den ursprünglichen Text handelt. Dies stellt uns schon vor die erste Herausforderung, da die meisten Machine Learning Methoden numerische Daten (in Matrixform) benötigen. Unsere erste Aufgabe ist es daher diese Daten in die entsprechende Form zu bringen. Dazu können wir verschiedene *[Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)* Methoden verwenden.","metadata":{}},{"cell_type":"code","source":"print(f\"{categories[y_train[0]]},\", y_train[0], \"\\n\", X_train[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2a) Vektorisierungsmethoden\nVektorisierung bedeutet nichts anderes als einen Text in Vektorform zu bringen. Dazu gibt es verschiedene Möglichkeiten. Die einfachste ist ein sogenannter [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer). Dabei wird aus dem Text-Datenset ein Vokabular erstellt und die einzelnen Terme werden für jeden Text gezählt. \n\nDer folgende Code bringt das ursprüngliche Set in Matrixform indem es den `CountVectorizer` verwendet. Die Vektorisierung hat ein Vokabular mit 52426 Einträgen erstellt.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nextractor = CountVectorizer()\nX_train_vec = extractor.fit_transform(X_train)\nprint(X_train_vec.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Leider erzeugt dieser Vektorizer auch sehr viele schlechte Features welche nur in einem oder zwei Texten vorkommen. Wir können den Parameter `min_df` erhöhen um zu erzwingen, dass jeder Term (*Token*) mindestens in `min_df` Texten vorkommen muss. Man kann einen Integer Wert verwenden oder einen Float zwischen 0.0 und 1.0, welcher den Prozentsatz angibt. Im folgenden Beispiel ist `min_df=5` und man kann sehen, dass das Vokabular schon etwas kleiner ist (11839 Einträge). ","metadata":{}},{"cell_type":"code","source":"extractor = CountVectorizer(min_df=5)\nX_train_vec = extractor.fit_transform(X_train)\nprint(X_train_vec.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wir haben immer noch das Problem, dass unsere Vektorisierung viele unnütze Features erzeugt. Hier ist ein Code, welcher die *Document Frequency* der einzelnen *Token* ausgibt.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef sort_dict(d):\n    return dict(sorted(d.items(), key=lambda x: x[1]))\n\ndef document_frequency(X_train, **kwargs):\n    \"\"\"\n    Calculates the document frequency of a given set. \n    \"\"\"\n    m = len(X_train)\n    extractor = TfidfVectorizer(**kwargs)\n    X_train_vec = extractor.fit_transform(X_train)\n    # inverse of idf\n    df = (1 + m) / np.exp(extractor.idf_ - 1) - 1 \n    df = df.astype(np.int64)\n    vocabulary = sort_dict(extractor.vocabulary_)  # sort by index\n    return sort_dict({k: df[i] for (i, k) in enumerate(vocabulary.keys())})  # sort by df\n\ndef print_df(df, n):\n    \"\"\"\n    Prints first `n` items in df.\n    \"\"\"\n    for token in list(df.items())[:n]:\n        print(token)\n\nprint_df(document_frequency(X_train, min_df=3), 40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Der `CountVectorizer` ist eine sehr einfache Methode der Vektorisierung. Wenn wir `min_df` erhöhen, dann wird das Datenset zwar kleiner, jedoch werden auch viele Terme ignoriert, welche potentielle Informationen enthalten. Wir werden eine etwas kompliziertere Methode verwenden: [*Term Frequency $\\times$ inverse Document Frequency*](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting). Die Implementierung findet sich in der Klasse `TfidfVectorizer`. Mit dieser Methode lassen sich oft bessere Modelle erstellen. Der folgende Code soll den Unterschied zwischen *Term Frequency* und *Document Frequency* noch einmal verdeutlichen.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom itertools import chain\nfrom collections import defaultdict\ndf = defaultdict(int)\n\ndoc1 = \"Cat Dog Raven Cat\"\ndoc2 = \"Raven Dog Dog Cow\"\n\ntf_doc1 = {term: count for (term, count) in zip(*np.unique(np.array(doc1.split(' ')), return_counts=True))}\ntf_doc2 = {term: count for (term, count) in zip(*np.unique(np.array(doc2.split(' ')), return_counts=True))}\n\nfor term in chain(tf_doc1.keys(), tf_doc2.keys()):\n    df[term] += 1  # \n\ndf = dict(df)\ndataframe = pd.DataFrame.from_records([tf_doc1, tf_doc2]).fillna(0).astype(int)\ndataframe = dataframe.append(df, ignore_index=True)\ndataframe.index = [\"tf_doc1\", \"tf_doc2\", \"df\"]\ndataframe\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wir können sehen, dass viele Features nicht besonders aussagekräftig sind. Jedoch beinhalten manche wertvolle Informationen. Zum Beispiel kann man annehmen, dass `120mph` ein Hinweis auf die Kategorie `rec.autos` oder `rec.motorcycles` ist. Anstatt diese Token einfach zu entfernen ist es sinnvoll sie vorher umzuwandeln. Zum Beispiel können wir `120mph` mit `__SPEED__` ersetzen. \nDer folgende Code benutzt eine eigene Implementierung des `tokenizer`. Dieser spaltet einen Text in seine Bestandteile auf und wir können jeden Term einzeln bearbeiten.\n\n- Schreibe eine weitere Regel, welche alle Nummern + `kwh, mhz, miles, k, mi, usd` u.s.w. mit `__UNIT__` ersetzt. Z.B.: `100kwh` mit `__UNIT__`.\n- Schreibe eine weitere Regel, welche alle Token die mindestens eine Nummer enthalten mit `__ALPHANUMERIC__` ersetzt.\n- Wie viele Werte wurden ersetzt? Dafür kannst du wieder die Funktion `document_frequency` von vorhin verwenden.","metadata":{}},{"cell_type":"code","source":"import re\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nreg_tokenizer = RegexpTokenizer(r'(?u)\\b\\w\\w+\\b')  # (?u) = unicode, \\b = word boundary \\w\\w+ = word with at least 2 characters\nlemmatizer = WordNetLemmatizer()\nstopwords_en = set(stopwords.words('english'))\n\n\ndef valid(token):\n    return (token not in stopwords_en and token != \"\")\n\nfloating_number = re.compile(r'^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?$')\ndef is_decimal(token):\n    return bool(floating_number.match(token))\n    \nint_number = re.compile(r'^[-+]?[0-9]+$')\ndef is_integer(token):\n    return bool(int_number.match(token))\n\ndef is_number(token):\n    return is_decimal(token) or is_integer(token)\n\ndef is_alphanumeric(token):\n    ... # ersetzt ... mit dem richtigen Code\n    \ndef is_year(token):\n    return len(token) == 4 and token.startswith((\"18\", \"19\", \"20\")) and is_integer(token)\n\nnumberunit = re.compile(r'^\\d+[a-zA-Z]+$')\ndef is_speed(token):\n    return bool(numberunit.match(token)) and token.endswith(\"mph\")\n\ndef is_unit(token):\n    ... # ersetzt ... mit dem richtigen Code\n\n\ndef preprocess_token(token):\n    token = token.strip(\"_\").lower()\n    if token == \"\": return \"\"\n\n    if is_year(token): return \"__YEAR__\"\n    if is_number(token): return \"__NUMBER__\"\n    if is_speed(token): return \"__SPEED__\"\n    # ... Füge deine Regeln hier hinzu\n    return token\n\ndef tokenizer(text):\n    text = reg_tokenizer.tokenize(text)\n    text = map(preprocess_token, text)\n    text = filter(valid, text)\n    return map(lemmatizer.lemmatize, text)\n\ndf = document_frequency(X_train, tokenizer=tokenizer, min_df=3)\nprint_df(df, 40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"__NUMBER__:\", df[\"__NUMBER__\"])\nprint(\"__SPEED__:\", df[\"__SPEED__\"])\nprint(\"__YEAR__:\", df[\"__YEAR__\"])\nprint()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2b) Kernel Methoden und SVMs für Textklassifikation\n- Erstelle jetzt eine Pipeline welche diese `tokenizer` verwendet. Die erste Stufe sollte ein `TfidfVectorizer` sein, gefolgt von einer `SVC`.\n- Benutze `GridSearchCV` um nach passenden Hyperparametern zu suchen (ignoriere den Parameter `gamma`, der Defaultwert ist gut genug für den Anfang). Vergleiche verschiedene Kerne (z.B.: `\"linear\", \"rbf\"` oder  die `cosine_similarity` Funktion). Verwende den `f1_score` als Metrik. Setze dazu `scoring=score` in `GridSearchCV`. Wenn die Trainingszeit zu lange ist, dann verwende einen niedrigeren Wert für `cv`. Du kannst auch den Wert `n_jobs` erhöhen.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import f1_score\n\ndef score(model, X, y):\n    y_pred = model.predict(X)\n    return f1_score(y, y_pred, average='macro')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Berechne die Wahrheitsmatrix deines Modelles anhand des Trainingssets. \n- Normalisiere die Matrix indem du durch die Summe der Zeilen dividierst und die Diagonale 0 setzt.\n- Plotte die normalisierte Matrix. Was kannst du sehen?","metadata":{}},{"cell_type":"code","source":"from sklearn.base import clone\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_confusion_matrix(y_true, y_pred):\n    fig, ax = plt.subplots(1,1, figsize=(10,10)) \n    norm_conf_mx = ... # Berechne die Wahrheitsmatrix und normalisiere!\n    ax.matshow(norm_conf_mx, cmap=plt.cm.gray)\n    ticks = list(range(11))\n    ax.set_xticks(ticks)\n    ax.set_yticks(ticks)\n    ax.set_xticklabels(categories, rotation='vertical')\n    ax.set_yticklabels(categories);\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Welchen F1-Score erzielst du auf dem Testset?","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Welche Kategorien werden `X_test[13], X_test[42], X_test[66]` und `X_test[333]` zugewiesen","metadata":{}},{"cell_type":"code","source":"print(categories[y_test[13]], \"\\n\", X_test[13])\nprint(\"--------------\")\nprint(categories[y_test[42]], \"\\n\", X_test[42])\nprint(\"--------------\")\nprint(categories[y_test[66]], \"\\n\", X_test[66])\nprint(\"--------------\")\nprint(categories[y_test[333]], \"\\n\", X_test[333])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"finale Version","metadata":{}}]}