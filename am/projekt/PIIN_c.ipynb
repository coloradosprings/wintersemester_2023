{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = 1,1,1,1\n",
    "NN = Sequential([\n",
    "    tf.keras.layers.Input((1,)),\n",
    "    tf.keras.layers.Dense(units = 32, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(units = 32, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(units = 1)\n",
    "])\n",
    "T = Sequential([\n",
    "    tf.keras.layers.Input((1,)),\n",
    "    tf.keras.layers.Dense(units = 32, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(units = 32, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(units = 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_vars1 = NN.trainable_variables\n",
    "trainable_vars2 = T.trainable_variables\n",
    "g = lambda x : np.cos(x)\n",
    "train_t = (np.array([0., 0.025, 0.475, 0.5, 0.525, 0.9, 0.95, 1., 1.05, 1.1, 1.4, 1.45, 1.5, 1.55, 1.6, 1.95, 2.])).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_loss(t,NN,T,g):\n",
    "    t = t.reshape(-1,1)\n",
    "    t = tf.constant(t, dtype = tf.float32)\n",
    "    t_0 = tf.zeros((1,1))\n",
    "    one = tf.ones((1,1))\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(t)\n",
    "        u = NN(t)\n",
    "        dnn = tape.gradient(u, t)\n",
    "        ddnn = tape.gradient(dnn,t)\n",
    "    ode_loss =  (a * ddnn + b * dnn + c * u - g(t))\n",
    "   \n",
    "    iv_loss = NN(t_0) # we preceed with one intital value loss y_prime\n",
    "    square_loss = T(t) *( tf.square(ode_loss) + tf.square(iv_loss) )\n",
    "\n",
    "    total_loss = tf.reduce_mean(square_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "competitive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    }
   ],
   "source": [
    "train_t = (np.array([0., 0.025, 0.475, 0.5, 0.525, 0.9, 0.95, 1., 1.05, 1.1, 1.4, 1.45, 1.5, 1.55, 1.6, 1.95, 2.])).reshape(-1, 1)\n",
    "train_loss_record1 = []\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "epochs = 300\n",
    "for _ in range(epochs):\n",
    "    with tf.GradientTape(persistent=True) as tape: \n",
    "        loss1 = shared_loss(train_t,NN,T,g)\n",
    "        loss2 =  -loss1\n",
    "    grad = tape.gradient(loss1, trainable_vars1)\n",
    "    optimizer.apply_gradients(zip(grad, trainable_vars1))\n",
    "    #discriminator\n",
    "    grad = tape.gradient(loss2, trainable_vars2)\n",
    "    optimizer.apply_gradients(zip(grad, trainable_vars2))\n",
    "    \n",
    "    train_loss_record1.append(loss1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002B7CD2E1430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002B7CD2E1430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002B7CD411790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002B7CD411790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 67ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = NN.predict(train_t)\n",
    "z_pred = T.predict(train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C_GradientDescent(tf.Module):\n",
    "\n",
    "  def __init__(self, learning_rate=1e-3):\n",
    "    # Initialize parameters\n",
    "    self.learning_rate = learning_rate\n",
    "    self.title = f\"Gradient descent optimizer: learning rate={self.learning_rate}\"\n",
    "\n",
    "  def apply_gradients(self, grads,hess, vars):\n",
    "    # Update variables\n",
    "    for grad,hess, var in zip(grads,hess, vars):\n",
    "      print(type(var))\n",
    "      print(type(hess))\n",
    "      print(type(grads))\n",
    "      var.assign_sub(self.learning_rate*grad + self.learning_rate**2  * (hess * grad))\n",
    "# dy_dx = gg.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x000002B7C0898EE0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x000002B7C0898EE0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x000002B7CD411A60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x000002B7CD411A60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "var1 = NN.trainable_variables[k]\n",
    "var2 = T.trainable_variables[k]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape2: \n",
    "    with tf.GradientTape(persistent=True) as tape1: \n",
    "        loss1 = shared_loss(train_t,NN,T,g)\n",
    "    grad = tape1.gradient(loss1, var1)\n",
    "Dxy = tape2.jacobian(grad,var2)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape4: \n",
    "    with tf.GradientTape(persistent=True) as tape3: \n",
    "        loss2 = -shared_loss(train_t,NN,T,g)\n",
    "    grad2 = tape3.gradient(loss2, var2)\n",
    "Dyx = tape4.jacobian(grad2,var1)\n",
    "\n",
    "\n",
    "    # grad2 = tape.gradient(loss2, trainable_vars2)\n",
    "    # Dyx = tape.gradient(grad2,trainable_vars1)\n",
    "\n",
    "# optimizer.apply_gradients(zip(grad, trainable_vars2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32, 32), dtype=float32, numpy=\n",
       "array([[[ 1.48003528e-06, -6.74346893e-07, -4.35832362e-06, ...,\n",
       "         -6.87902457e-07,  1.78457105e-06,  2.73893420e-06],\n",
       "        [ 1.97296984e-07, -1.56123306e-07, -5.85506882e-07, ...,\n",
       "         -5.72679184e-08,  3.09878601e-07,  3.34831299e-07],\n",
       "        [-2.23069492e-08,  4.16340612e-07,  7.23300673e-08, ...,\n",
       "         -2.01396261e-07, -4.67658111e-07,  1.58435626e-07],\n",
       "        ...,\n",
       "        [ 1.57482187e-08, -2.02076308e-07, -9.41862766e-08, ...,\n",
       "          9.30471558e-08,  2.32027929e-07, -3.69569264e-08],\n",
       "        [-2.19159210e-06,  5.30763941e-07,  4.65573885e-06, ...,\n",
       "          1.21259893e-06, -2.07354810e-06, -3.09521783e-06],\n",
       "        [-6.44952343e-06, -4.09700306e-07,  1.82010644e-05, ...,\n",
       "          4.72277225e-06, -4.11779592e-06, -1.30913304e-05]],\n",
       "\n",
       "       [[-4.84378745e-07,  5.57237627e-07,  1.74960667e-06, ...,\n",
       "          5.85318389e-08, -9.60098532e-07, -9.42032329e-07],\n",
       "        [-5.31573789e-08,  1.24472280e-07,  2.08342911e-07, ...,\n",
       "         -2.61617785e-08, -1.74598028e-07, -8.24142461e-08],\n",
       "        [-4.90440470e-08, -2.91069853e-07,  9.56444524e-08, ...,\n",
       "          1.84979584e-07,  2.82460036e-07, -2.15854925e-07],\n",
       "        ...,\n",
       "        [ 4.65966821e-08,  1.55854011e-07, -7.47629798e-08, ...,\n",
       "         -1.12320166e-07, -1.38027190e-07,  1.33670738e-07],\n",
       "        [ 1.35017103e-06, -1.32216201e-07, -3.05878552e-06, ...,\n",
       "         -8.53985455e-07,  1.07272342e-06,  2.13141766e-06],\n",
       "        [ 3.08732638e-06,  5.81671884e-08, -9.39442634e-06, ...,\n",
       "         -2.20776610e-06,  2.14402780e-06,  6.65018251e-06]],\n",
       "\n",
       "       [[-2.08970576e-07,  5.60014470e-08,  4.79392838e-07, ...,\n",
       "          1.13802400e-07, -2.04824943e-07, -3.16208968e-07],\n",
       "        [-2.82388868e-08,  1.11092291e-08,  6.45494680e-08, ...,\n",
       "          1.35239837e-08, -3.15128155e-08, -4.08006287e-08],\n",
       "        [ 3.79744591e-09, -3.21618359e-08, -2.26508234e-09, ...,\n",
       "          1.43629935e-08,  3.72861280e-08, -1.41617242e-08],\n",
       "        ...,\n",
       "        [-1.56745923e-08,  9.27149379e-09,  3.60272452e-08, ...,\n",
       "          5.89195004e-09, -2.08663469e-08, -2.12183338e-08],\n",
       "        [ 1.19967325e-07, -7.28904865e-08, -2.22789453e-07, ...,\n",
       "         -4.26126689e-08,  1.59962894e-07,  1.26123695e-07],\n",
       "        [ 5.81964173e-07, -8.50543813e-08, -1.34959896e-06, ...,\n",
       "         -3.54331888e-07,  4.94001711e-07,  9.25853783e-07]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-7.16722184e-08,  2.81400133e-07,  4.04641128e-07, ...,\n",
       "         -9.10891629e-08, -3.62826881e-07, -1.36290566e-07],\n",
       "        [-6.10032913e-10,  6.10324946e-08,  3.26678062e-08, ...,\n",
       "         -3.05583328e-08, -6.76854057e-08,  8.88540086e-09],\n",
       "        [-4.58020963e-08, -1.34136997e-07,  1.04082289e-07, ...,\n",
       "          1.01313738e-07,  1.13945468e-07, -1.42190714e-07],\n",
       "        ...,\n",
       "        [ 3.41457920e-08,  7.20668680e-08, -6.88872817e-08, ...,\n",
       "         -6.07109314e-08, -5.49533041e-08,  8.61592611e-08],\n",
       "        [ 5.12555744e-07, -3.55328389e-08, -1.18331468e-06, ...,\n",
       "         -3.32457944e-07,  3.92082598e-07,  8.31187208e-07],\n",
       "        [ 9.50565266e-07, -1.07073902e-07, -3.07588766e-06, ...,\n",
       "         -6.19651587e-07,  8.01957981e-07,  2.10661619e-06]],\n",
       "\n",
       "       [[ 2.08680581e-06, -9.29674911e-07, -5.69270742e-06, ...,\n",
       "         -9.68356403e-07,  2.47792923e-06,  3.57234831e-06],\n",
       "        [ 2.73588682e-07, -2.06973908e-07, -7.48710931e-07, ...,\n",
       "         -8.26205380e-08,  4.17238113e-07,  4.27178577e-07],\n",
       "        [ 8.70483063e-09,  5.59937405e-07, -4.49192612e-08, ...,\n",
       "         -2.98836540e-07, -6.00469434e-07,  3.12217537e-07],\n",
       "        ...,\n",
       "        [ 5.77176316e-08, -2.40634222e-07, -1.84424820e-07, ...,\n",
       "          8.46106971e-08,  3.02583572e-07,  8.83122198e-09],\n",
       "        [-2.50040807e-06,  9.41001588e-07,  5.04977015e-06, ...,\n",
       "          1.20112645e-06, -2.72055149e-06, -3.18842467e-06],\n",
       "        [-8.15427484e-06,  1.06129846e-07,  2.19607828e-05, ...,\n",
       "          5.61616753e-06, -5.84755026e-06, -1.55409489e-05]],\n",
       "\n",
       "       [[ 7.54233497e-07, -6.36386176e-07, -2.26142015e-06, ...,\n",
       "         -1.98941052e-07,  1.22837469e-06,  1.27580779e-06],\n",
       "        [ 8.78708022e-08, -1.38197990e-07, -2.71660156e-07, ...,\n",
       "          1.00195763e-08,  2.12868727e-07,  1.21905785e-07],\n",
       "        [ 5.82663517e-08,  3.32639530e-07, -1.40368854e-07, ...,\n",
       "         -2.13639481e-07, -3.20396708e-07,  2.67532698e-07],\n",
       "        ...,\n",
       "        [-1.89292049e-08, -1.59473487e-07,  1.85827993e-08, ...,\n",
       "          9.54760395e-08,  1.60781184e-07, -9.47383754e-08],\n",
       "        [-1.36804056e-06,  2.88631099e-07,  2.98083387e-06, ...,\n",
       "          7.81286531e-07, -1.25061911e-06, -2.00316322e-06],\n",
       "        [-3.63071649e-06,  2.35724201e-07,  1.02944705e-05, ...,\n",
       "          2.41667271e-06, -2.82571932e-06, -7.16813292e-06]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.matvec(Dxy, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains1(var):\n",
    "    for i in var.shape:\n",
    "        if i == 1:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_params = np.max(var1.shape)\n",
    "\n",
    "# grad = tf.reshape(grad, [n_params, 1])\n",
    "# grad2 = tf.reshape(grad2, [n_params, 1])\n",
    "# Dxy = tf.reshape(Dxy, [n_params, n_params])\n",
    "# Dyx = tf.reshape(Dxy, [n_params, n_params])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 10e-2\n",
    "dim = tf.squeeze(var1).shape.__len__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0,32):\n",
    "    grad_k = tf.reshape(grad[k], [n_params, 1])\n",
    "    Dxy[0][k] @ grad_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[0.00022872]]]], dtype=float32)>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tf.tensordot(Dxy,grad2,dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2  = grad - mu *  tf.linalg.matvec(Dxy,grad2,dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "M1 = tf.linalg.inv(tf.eye(32) - mu**2 *  tf.tensordot(Dxy,Dyx,dim))\n",
    "M2  = grad - mu *  tf.linalg.matvec(Dxy,grad2,dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "train_loss_record1 = []\n",
    "epochs = 300\n",
    "for k in range(epochs):\n",
    "    with tf.GradientTape(persistent=True) as tape: \n",
    "        loss1 = shared_loss(train_t,NN,T,g)\n",
    "        loss2 =  -loss1\n",
    "    grad = tape.gradient(loss1, trainable_vars1)\n",
    "    optimizer.apply_gradients(zip(grad, trainable_vars1))\n",
    "    #discriminator\n",
    "    grad = tape.gradient(loss2, trainable_vars2)\n",
    "    optimizer.apply_gradients(zip(grad, trainable_vars2))\n",
    "    \n",
    "    train_loss_record1.append(loss1)\n",
    "    # print(\"k : \", k)\n",
    "    # print(shared_loss(train_t,NN,T,g))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(0,30):\n",
    "    NN = Sequential([\n",
    "        tf.keras.layers.Input((1,)),\n",
    "        tf.keras.layers.Dense(units = 32, activation = 'tanh'),\n",
    "        tf.keras.layers.Dense(units = 32, activation = 'tanh'),\n",
    "        tf.keras.layers.Dense(units = 1)\n",
    "    ])\n",
    "    T = Sequential([\n",
    "        tf.keras.layers.Input((1,)),\n",
    "        tf.keras.layers.Dense(units = 32, activation = 'tanh'),\n",
    "        tf.keras.layers.Dense(units = 32, activation = 'tanh'),\n",
    "        tf.keras.layers.Dense(units = 1)\n",
    "    ])\n",
    "    train_loss_record1 = []\n",
    "    epochs = 300\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "    trainable_vars1 = NN.trainable_variables\n",
    "    trainable_vars2 = T.trainable_variables\n",
    "    g = lambda x : np.cos(x)\n",
    "    train_t = (np.array([0., 0.025, 0.475, 0.5, 0.525, 0.9, 0.95, 1., 1.05, 1.1, 1.4, 1.45, 1.5, 1.55, 1.6, 1.95, 2.])).reshape(-1, 1)\n",
    "    for k in range(epochs):\n",
    "        with tf.GradientTape(persistent=True) as tape: \n",
    "            loss1 = shared_loss(train_t,NN,T,g)\n",
    "            loss2 =  -loss1\n",
    "        grad = tape.gradient(loss1, trainable_vars1)\n",
    "        optimizer.apply_gradients(zip(grad, trainable_vars1))\n",
    "        #discriminator\n",
    "        grad = tape.gradient(loss2, trainable_vars2)\n",
    "        optimizer.apply_gradients(zip(grad, trainable_vars2))\n",
    "        \n",
    "        train_loss_record1.append(loss1)\n",
    "        # print(\"k : \", k)\n",
    "        # print(shared_loss(train_t,NN,T,g))'\n",
    "    if train_loss_record1[150] < train_loss_record1[290]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.04390478>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.05097301>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.05782873>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.06445987>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.07087475>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.07709508>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0831492>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.089066625>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09487406>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10059358>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10624188>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11183081>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11736807>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.122858256>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12830393>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1337062>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13906534>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1443813>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14965354>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15488163>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16006492>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16520277>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17029439>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17533907>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18033594>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18528423>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19018288>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19503096>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19982755>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20457159>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20926204>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21389785>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21847789>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.22300106>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.22746632>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.23187254>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.23621862>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2405034>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2447258>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.24888471>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.25297904>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.25700766>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.26096952>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.26486343>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.26868844>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.27244332>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.27612713>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.27973866>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2832771>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2867411>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29012987>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29344228>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29667735>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29983398>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3029113>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.30590838>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.30882424>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.31165785>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.31440833>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3170749>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3196566>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3221526>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.32456204>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.32688415>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.32911822>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33126348>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33331916>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33528456>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33715916>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33894226>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3406333>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3422317>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.343737>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3451488>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34646663>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34769002>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3488188>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34985256>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35079116>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35163438>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35238197>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3530339>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3535902>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3540508>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35441583>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35468534>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3548595>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35493857>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35492277>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3548126>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35460827>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35431036>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3539193>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3534357>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35286018>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35219344>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35143623>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35058928>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34965354>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34862998>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3475194>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.346323>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34504178>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34367722>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34223008>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34070194>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33909392>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33740774>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3356445>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33380607>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33189365>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.32990915>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.32785407>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3257303>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3235396>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.32128376>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.31896478>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.31658444>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.31414473>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3116479>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.30909568>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.30649048>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.30383456>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.30112982>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29837865>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29558328>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29274595>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.28986916>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.28695512>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.28400603>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.28102458>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.278013>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.27497378>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.27190915>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2688216>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.26571345>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2625873>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.25944537>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.25628996>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.25312352>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.24994834>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.24676657>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.24358065>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.24039274>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2372049>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.23401935>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.23083821>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2276634>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.224497>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2213408>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21819678>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21506672>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21195218>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20885503>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20577672>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2027189>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19968286>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19667016>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19368188>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1907194>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18778391>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18487647>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18199798>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17914955>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17633186>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17354596>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17079234>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16807169>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16538474>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1627319>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1601137>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1575305>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15498272>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15247053>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14999425>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14755401>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14515007>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14278233>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14045087>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13815586>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13589697>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1336744>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13148786>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12933727>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12722231>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12514293>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12309898>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12108984>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11911565>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11717566>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11526996>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.113397904>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.111559376>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.109753765>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10798087>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10624026>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1045317>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10285476>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1012097>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.099597216>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09802008>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09648629>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09501805>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09367558>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.092624545>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.092323616>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09404898>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10143633>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.124662034>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19369125>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.38978815>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.95489347>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.3886468>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.79627>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=9.694176>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=12.217032>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=7.0973635>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.346676>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2611432>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.6137006>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.413498>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3471816>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3190452>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.30292308>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29094723>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2806689>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.27128243>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.26248047>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.25414827>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.24622211>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.23867118>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2314685>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.22459637>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21803606>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21177319>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2057923>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20008042>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19462427>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18941209>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18443191>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17967315>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17512478>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1707771>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16662012>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16264506>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15884279>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1552048>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15172307>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14838982>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14519742>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14213885>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13920733>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1363963>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13369939>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13111074>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12862456>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1262356>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.123938605>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.121728785>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11960132>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.117552005>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11557662>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11367124>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.111832075>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11005594>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10833925>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.106679365>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10507324>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10351879>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10201349>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10055668>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09914684>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09778569>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09647511>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09522332>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09404241>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09296059>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.092024945>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.091329716>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.091044664>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.091495946>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.093304984>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.097646184>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.106866345>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12531926>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16209306>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2331444>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.37274528>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.63015485>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1057467>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.8321037>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.8349319>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.5259829>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.608961>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.7614825>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.7689314>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0352551>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.6078542>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.39166698>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.28010646>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_record1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1)\n",
    "axs.set_xlabel(\"t\")\n",
    "axs.set_ylabel(\"pred\")\n",
    "axs.plot(train_loss_record1,\"b\")\n",
    "axs.spines['top'].set_visible(False)\n",
    "axs.spines['right'].set_visible(False)\n",
    "axs.plot([0 for k in range(300)],\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- gradxf - mu * Dxyf * gradyf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x000002B7CC078D30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x000002B7CC078D30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape0: \n",
    "    with tf.GradientTape(persistent=True) as tape1: \n",
    "        loss2 = -shared_loss(train_t,NN,T,g)\n",
    "    grad1 = tape1.gradient(loss2, var2)\n",
    "Dyx = tape0.jacobian(grad1,var1)\n",
    "\n",
    "    \n",
    "    \n",
    "with tf.GradientTape(persistent=True) as tape4: \n",
    "    with tf.GradientTape(persistent=True) as tape3: \n",
    "        loss2 = shared_loss(train_t,NN,T,g)\n",
    "    with tf.GradientTape(persistent=True) as tape5: \n",
    "        loss3 = shared_loss(train_t,NN,T,g)\n",
    "    grad2 = tape3.gradient(loss2, var1)\n",
    "    grad3 = tape5.gradient(loss3, var2)\n",
    "    \n",
    "    \n",
    "Dxy_gradyf = tape2.gradient(grad,var2,output_gradients = grad1) #Hesse Vektor produkt\n",
    "# für Hesse*Hesse produkt spaltenweise Hess*gradient produkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in Dxy:\n",
    "#     print(k.shape)\n",
    "# grad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGA_iteration(mu = 1e-03):\n",
    "    for k in range(0,T.trainable_variables.__len__()):\n",
    "        # k = 4\n",
    "        \n",
    "        \n",
    "        var1 = NN.trainable_variables[k]\n",
    "        var2 = T.trainable_variables[k]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape2: \n",
    "            \n",
    "            with tf.GradientTape(persistent=True) as tape1: \n",
    "                loss = shared_loss(train_t,NN,T,g)\n",
    "            with tf.GradientTape(persistent=True) as tape0: \n",
    "                loss1 = shared_loss(train_t,NN,T,g)\n",
    "            grad = tape1.gradient(loss, var1)\n",
    "            grad1 = tape0.gradient(loss1, var2)\n",
    "            \n",
    "            \n",
    "        with tf.GradientTape(persistent=True) as tape4: \n",
    "            with tf.GradientTape(persistent=True) as tape3: \n",
    "                loss2 = shared_loss(train_t,NN,T,g)\n",
    "            with tf.GradientTape(persistent=True) as tape5: \n",
    "                loss3 = shared_loss(train_t,NN,T,g)\n",
    "            grad2 = tape3.gradient(loss2, var1)\n",
    "            grad3 = tape5.gradient(loss3, var2)\n",
    "            \n",
    "            \n",
    "        Dxy_gradyf = tape2.gradient(grad,var2,output_gradients = grad1) #Hesse Vektor produkt\n",
    "        # tf.print(Dxx_gradxf)\n",
    "\n",
    "        Dyx_gradxg = tape4.gradient(grad2,var1,output_gradients = grad2) #Hesse Vektor produkt\n",
    "        \n",
    "        var1.assign_sub(-mu * grad - mu *  Dxy_gradyf )\n",
    "        var2.assign_sub(-mu * grad2 - mu *  Dyx_gradxg )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k :  0\n",
      "tf.Tensor(5.979331, shape=(), dtype=float32)\n",
      "k :  1\n",
      "tf.Tensor(-1.8480353, shape=(), dtype=float32)\n",
      "k :  2\n",
      "tf.Tensor(-1.577327, shape=(), dtype=float32)\n",
      "k :  3\n",
      "tf.Tensor(3.9786193, shape=(), dtype=float32)\n",
      "k :  4\n",
      "tf.Tensor(-381671830000000.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "train_loss_record1 = []\n",
    "for k in range(epochs):\n",
    "    SGA_iteration()\n",
    "    print(\"k : \", k)\n",
    "    print(shared_loss(train_t,NN,T,g))\n",
    "    # train_loss_record1.append(shared_loss(train_t,NN,T,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=7.6302238>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.896523e+19>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_record1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OGDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/76093944/vjp-and-jvp-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         loss1 \u001b[38;5;241m=\u001b[39m shared_loss(train_t,NN,T,g)\n\u001b[0;32m      8\u001b[0m     grad \u001b[38;5;241m=\u001b[39m tape1\u001b[38;5;241m.\u001b[39mgradient(loss1, var1)\n\u001b[1;32m----> 9\u001b[0m Dxy_gradf \u001b[38;5;241m=\u001b[39m tape2\u001b[38;5;241m.\u001b[39mgradient(grad,var2,output_gradients \u001b[38;5;241m=\u001b[39m \u001b[43mgrad2\u001b[49m)\n\u001b[0;32m     10\u001b[0m Dxx_gradf \u001b[38;5;241m=\u001b[39m tape2\u001b[38;5;241m.\u001b[39mgradient(grad,var1,output_gradients \u001b[38;5;241m=\u001b[39m grad)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape(persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tape4: \n",
      "\u001b[1;31mNameError\u001b[0m: name 'grad2' is not defined"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "var1 = NN.trainable_variables[k]\n",
    "var2 = T.trainable_variables[k]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape2: \n",
    "    with tf.GradientTape(persistent=True) as tape1: \n",
    "        loss1 = shared_loss(train_t,NN,T,g)\n",
    "    grad = tape1.gradient(loss1, var1)\n",
    "Dxy_gradf = tape2.gradient(grad,var2,output_gradients = grad2)\n",
    "Dxx_gradf = tape2.gradient(grad,var1,output_gradients = grad)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape4: \n",
    "    with tf.GradientTape(persistent=True) as tape3: \n",
    "        loss2 = -shared_loss(train_t,NN,T,g)\n",
    "    grad2 = tape3.gradient(loss2, var2)\n",
    "    \n",
    "# Dyx = tape4.jacobian(grad2,var1)\n",
    "A_v = tape4.gradient(grad2,var1,output_gradients = grad2)\n",
    "\n",
    "var1 = grad - mu *  Dxy_gradf  + mu * tf.linalg.matvec(Dxy,grad)\n",
    "\n",
    "\n",
    "    # grad2 = tape.gradient(loss2, trainable_vars2)\n",
    "    # Dyx = tape.gradient(grad2,trainable_vars1)\n",
    "\n",
    "# optimizer.apply_gradients(zip(grad, trainable_vars2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OGDA_iteration(mu = 1e-03):\n",
    "    for k in range(0,T.trainable_variables.__len__()):\n",
    "        # k = 4\n",
    "        \n",
    "        \n",
    "        var1 = NN.trainable_variables[k]\n",
    "        var2 = T.trainable_variables[k]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape2: \n",
    "            \n",
    "            with tf.GradientTape(persistent=True) as tape1: \n",
    "                loss1 = shared_loss(train_t,NN,T,g)\n",
    "            with tf.GradientTape(persistent=True) as tape0: \n",
    "                loss = shared_loss(train_t,NN,T,g)\n",
    "            grad = tape1.gradient(loss1, var1)\n",
    "            grad0 = tape0.gradient(loss, var2)\n",
    "            \n",
    "            \n",
    "        with tf.GradientTape(persistent=True) as tape4: \n",
    "            with tf.GradientTape(persistent=True) as tape3: \n",
    "                loss2 = -shared_loss(train_t,NN,T,g)\n",
    "            with tf.GradientTape(persistent=True) as tape5: \n",
    "                loss3 = -shared_loss(train_t,NN,T,g)\n",
    "            grad2 = tape3.gradient(loss2, var2)\n",
    "            grad3 = tape5.gradient(loss3, var1)\n",
    "            \n",
    "        Dxy_gradyf = tape2.gradient(grad,var2,output_gradients = grad0) #Hesse Vektor produkt\n",
    "        # tf.print(Dxx_gradxf)\n",
    "        Dxx_gradxf = tape2.gradient(grad,var1,output_gradients = grad) # HEsse Vektor Product#noch zu überprüfen\n",
    "\n",
    "        # Dyy_gradyg = tape4.gradient(grad2,var2,output_gradients = grad2) # HEsse Vektor Product#noch zu überprüfen\n",
    "\n",
    "        Dyx_gradxg = tape4.gradient(grad2,var1,output_gradients = grad3) #Hesse Vektor produkt\n",
    "        \n",
    "        print\n",
    "        #berechne Dyy_gradyg mit forward \n",
    "        with tf.GradientTape() as grad_tape:\n",
    "            with  tf.autodiff.ForwardAccumulator (\n",
    "                var2, grad2) as acc:\n",
    "                loss = -shared_loss(train_t,NN,T,g)\n",
    "        Dyy_gradyg = grad_tape.gradient(acc.jvp(loss), var2)\n",
    "        # tf.print(Dyy_gradyg)\n",
    "        # tf.print(var2)\n",
    "\n",
    "\n",
    "            # Dyxg_gradg = tape4.gradient(grad2,var1,output_gradients = grad2)\n",
    "            # Dyy_gradxg = tape4\n",
    "\n",
    "            # var1 = grad - mu *  Dxy_gradf  + mu * Dxx_gradf\n",
    "            # var2 = grad2 - mu * D\n",
    "            #versuche es mit forward acc\n",
    "        var1.assign_sub(-grad - mu *  Dxy_gradyf  + mu * Dxx_gradxf)\n",
    "        var2.assign_sub(-grad2 - mu *  Dyx_gradxg  + mu * Dyy_gradyg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGA_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop sgda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k :  0\n",
      "k :  1\n",
      "k :  2\n",
      "k :  3\n",
      "k :  4\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "train_loss_record1 = []\n",
    "for k in range(epochs):\n",
    "    SGA_iteration()\n",
    "    print(\"k : \", k)\n",
    "    train_loss_record1.append(shared_loss(train_t,NN,T,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=14752.608>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_record1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward\n",
    "\n",
    "with tf.autodiff.ForwardAccumulator (\n",
    "    var2, grad2) as acc:\n",
    "    with tf.GradientTape() as grad_tape:\n",
    "        loss = shared_loss(train_t,NN,T,g)\n",
    "grad1 = grad_tape.gradient(loss, var2)\n",
    "Dyy = acc.jvp(grad1)\n",
    "grad1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as grad_tape:\n",
    "# grad_tape.watch(h1.trainable_variables)\n",
    "    # loss = -shared_loss(train_t,NN,T,g)\n",
    "    with  tf.autodiff.ForwardAccumulator (\n",
    "        var2, grad2) as acc:\n",
    "        loss = -shared_loss(train_t,NN,T,g)\n",
    "\n",
    "U = grad_tape.gradient(acc.jvp(loss), var2)\n",
    "#forward wirft keinen None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "var2 = T.trainable_variables[5]\n",
    "with tf.GradientTape(persistent=True) as tape4: \n",
    "    tape4.watch(var2)\n",
    "    with tf.GradientTape(persistent=True) as tape3: \n",
    "        loss = -shared_loss(train_t,NN,T,g)\n",
    "    grad2 = tape3.gradient(loss, var2)\n",
    "# print(type(grad2))\n",
    "Z = tape4.gradient(grad2,var2,output_gradients = grad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00, -1.4901161e-08,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.4901161e-08,\n",
       "       -5.8207661e-11,  0.0000000e+00, -1.4901161e-08,  0.0000000e+00,\n",
       "       -1.1641532e-10, -1.4901161e-08, -3.7252903e-09,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  3.7252903e-09,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        1.8626451e-09,  0.0000000e+00,  0.0000000e+00, -7.4505806e-09],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U - Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(t):\n",
    "    return np.cos(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "[-0.198037535 0.149052531 0.0767920911 ... 0.00750894845 0.0101512596 -0.00354607403]\n"
     ]
    }
   ],
   "source": [
    "var2 = NN.trainable_variables[3]\n",
    "with tf.GradientTape(persistent=True) as tape4: \n",
    "    with tf.GradientTape(persistent=True) as tape3: \n",
    "        loss = -shared_loss(train_t,NN,T,g)\n",
    "    grad2 = tape3.gradient(loss, var2)\n",
    "print(type(grad2))\n",
    "tf.print(tape4.gradient(grad2,var2))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setze es zu null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[-4.6110000e-07,  3.7966242e-05, -4.1607189e-05, ...,\n",
       "         2.6117861e-05,  1.8781768e-05, -2.6791133e-05],\n",
       "       [ 1.8468677e-07, -1.7177117e-05,  1.8871104e-05, ...,\n",
       "        -1.1814228e-05, -8.4978428e-06,  1.2170377e-05],\n",
       "       [ 1.7577227e-06, -8.4219442e-05,  9.0861446e-05, ...,\n",
       "        -5.8007514e-05, -4.1651398e-05,  5.7917787e-05],\n",
       "       ...,\n",
       "       [ 8.2121375e-07, -5.6645764e-05,  6.1817969e-05, ...,\n",
       "        -3.8980816e-05, -2.8020357e-05,  3.9698290e-05],\n",
       "       [ 1.1281536e-08, -1.0837645e-06,  1.1913679e-06, ...,\n",
       "        -7.4536513e-07, -5.3616446e-07,  7.6863489e-07],\n",
       "       [-9.2735581e-07,  6.0929073e-05, -6.6406414e-05, ...,\n",
       "         4.1932632e-05,  3.0138432e-05, -4.2609481e-05]], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1.assign_sub(grad - mu *  Dxy_gradyf  + mu * Dxx_gradxf)\n",
    "var2.assign_sub(grad - mu *  Dyx_gradxg  + mu * Dyy_gradyg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(32,) dtype=float32, numpy=\n",
       "array([-0.01087814, -0.00333859,  0.00283779,  0.00824017, -0.00798694,\n",
       "        0.00863228, -0.01211496,  0.01969597,  0.00918963,  0.00796708,\n",
       "       -0.00209234, -0.00804568,  0.01245896,  0.00431448,  0.00535099,\n",
       "        0.01060803,  0.01284291, -0.01190958, -0.0016384 , -0.00893993,\n",
       "       -0.01216437, -0.02015602, -0.00454608, -0.00563179, -0.01800698,\n",
       "        0.00070412, -0.00197394, -0.0015933 , -0.00205856,  0.00761206,\n",
       "        0.02295514,  0.00234789], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1.assign_sub(grad - mu *  Dxy_gradyf  + mu * Dxx_gradxf)\n",
    "var2.assign_sub(grad - mu *  Dyx_gradxg  + mu * Dyy_gradyg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as inner_tape:\n",
    "      logits = h1(images, training=True)\n",
    "      loss = tf.compat.v1.losses.softmax_cross_entropy(\n",
    "          logits=logits, onehot_labels=labels)\n",
    "    grads = inner_tape.gradient(loss, h1.trainable_variables)\n",
    "  return outer_tape.gradient(\n",
    "      grads, h1.trainable_variables, output_gradients=vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape: \n",
    "    grads = tape.gradient(grad,var2,output_gradients = grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad - mu *  tf.linalg.matvec(Dxy,grad2) + mu * tf.linalg.matvec(Dxy,grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[ 3.1644796e-04, -2.6273359e-05,  1.4123552e-04, ...,\n",
       "         5.2533165e-04, -8.4452091e-05, -2.9865134e-04],\n",
       "       [ 8.7899028e-04, -7.0649476e-05,  3.8539164e-04, ...,\n",
       "         1.4516520e-03, -2.3058898e-04, -8.3000376e-04],\n",
       "       [-8.9740520e-04,  7.2051494e-05, -3.9285363e-04, ...,\n",
       "        -1.4815319e-03,  2.3510943e-04,  8.4739702e-04],\n",
       "       ...,\n",
       "       [-5.9449417e-04,  4.8763224e-05, -2.6404811e-04, ...,\n",
       "        -9.8534022e-04,  1.5786031e-04,  5.6118850e-04],\n",
       "       [ 6.0653838e-04, -4.9715196e-05,  2.6931131e-04, ...,\n",
       "         1.0052011e-03, -1.6100633e-04, -5.7256542e-04],\n",
       "       [-1.0105843e-03,  8.2312232e-05, -4.3471943e-04, ...,\n",
       "        -1.6639642e-03,  2.6158770e-04,  9.5373206e-04]], dtype=float32)>"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.tensordot(Dxy,grad,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def jacobian_transpose_vec(ys, xs, vs):\n",
    "  dydxs = tf.gradients(ys, xs, grad_ys=vs, stop_gradients=xs)\n",
    "  dydxs = [\n",
    "      tf.zeros_like(x) if dydx is None else dydx for x, dydx in zip(xs, dydxs)\n",
    "  ]\n",
    "  return dydxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fall n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.0000044], dtype=float32)>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = 1/(1 - mu**2 *  Dxy[0] * Dyx[0])\n",
    "M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[0.01608469]]]], dtype=float32)>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(1,) dtype=float32, numpy=array([0.06397489], dtype=float32)>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = 1/(1 - mu**2 *  Dxy[0] * Dyx[0])\n",
    "M2  = grad[0] - mu *  Dxy[0] * grad[0]\n",
    "var1.assign_sub(mu*M1 * M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1.shape == [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sgda git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'Optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     dot_list\u001b[38;5;241m.\u001b[39mappend(tf\u001b[38;5;241m.\u001b[39mreduce_sum(xx \u001b[38;5;241m*\u001b[39m yy))\n\u001b[0;32m     33\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39madd_n(dot_list)\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSymplecticOptimizer\u001b[39;00m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOptimizer\u001b[49m):\n\u001b[0;32m     37\u001b[0m   \u001b[38;5;124;03m\"\"\"Optimizer that corrects for rotational components in gradients.\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     40\u001b[0m                learning_rate,\n\u001b[0;32m     41\u001b[0m                reg_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m,\n\u001b[0;32m     42\u001b[0m                use_signs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     43\u001b[0m                use_locking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m                name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymplectic_optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'"
     ]
    }
   ],
   "source": [
    "#@title Defining the SGA Optimiser\n",
    "def jacobian_transpose_vec(ys, xs, vs):\n",
    "  dydxs = tf.gradients(ys, xs, grad_ys=vs, stop_gradients=xs)\n",
    "  dydxs = [\n",
    "      tf.zeros_like(x) if dydx is None else dydx for x, dydx in zip(xs, dydxs)\n",
    "  ]\n",
    "  return dydxs\n",
    "def list_divide_scalar(xs, y):\n",
    "  return [x / y for x in xs]\n",
    "\n",
    "\n",
    "def list_subtract(xs, ys):\n",
    "  return [x - y for (x, y) in zip(xs, ys)]\n",
    "\n",
    "\n",
    "#\n",
    "# def jacobian_vec(ys, xs, vs):\n",
    "#   return kfac.utils.fwd_gradients(\n",
    "#       ys, xs, grad_xs=vs, stop_gradients=xs)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "def jacobian_vec(ys, xs, vs):\n",
    "  return tf.autodiff.ForwardAccumulator(\n",
    "      ys, xs, grad_xs=vs, stop_gradients=xs)\n",
    "\n",
    "\n",
    "def _dot(x, y):\n",
    "  dot_list = []\n",
    "  for xx, yy in zip(x, y):\n",
    "    dot_list.append(tf.reduce_sum(xx * yy))\n",
    "  return tf.add_n(dot_list)\n",
    "\n",
    "\n",
    "class SymplecticOptimizer(tf.train.Optimizer):\n",
    "  \"\"\"Optimizer that corrects for rotational components in gradients.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate,\n",
    "               reg_params=1.,\n",
    "               use_signs=True,\n",
    "               use_locking=False,\n",
    "               name='symplectic_optimizer'):\n",
    "    super(SymplecticOptimizer, self).__init__(\n",
    "        use_locking=use_locking, name=name)\n",
    "    self._gd = tf.train.RMSPropOptimizer(learning_rate)\n",
    "    self._reg_params = reg_params\n",
    "    self._use_signs = use_signs\n",
    "\n",
    "  def compute_gradients(self,\n",
    "                        loss,\n",
    "                        var_list=None,\n",
    "                        gate_gradients=tf.train.Optimizer.GATE_OP,\n",
    "                        aggregation_method=None,\n",
    "                        colocate_gradients_with_ops=False,\n",
    "                        grad_loss=None):\n",
    "    return self._gd.compute_gradients(loss, var_list, gate_gradients,\n",
    "                                      aggregation_method,\n",
    "                                      colocate_gradients_with_ops, grad_loss)\n",
    "\n",
    "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
    "    grads, vars_ = zip(*grads_and_vars)\n",
    "    n = len(vars_)\n",
    "    h_v = jacobian_vec(grads, vars_, grads)\n",
    "    ht_v = jacobian_transpose_vec(grads, vars_, grads)\n",
    "    at_v = list_divide_scalar(list_subtract(ht_v, h_v), 2.)\n",
    "    if self._use_signs:\n",
    "      grad_dot_h = _dot(grads, ht_v)\n",
    "      at_v_dot_h = _dot(at_v, ht_v)\n",
    "      mult = grad_dot_h * at_v_dot_h\n",
    "      lambda_ = tf.sign(mult / n + 0.1) * self._reg_params\n",
    "    else:\n",
    "      lambda_ = self._reg_params\n",
    "    apply_vec = [(g + lambda_ * ag, x)\n",
    "                 for (g, ag, x) in zip(grads, at_v, vars_)\n",
    "                 if at_v is not None]\n",
    "    return self._gd.apply_gradients(apply_vec, global_step, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tf.tensordot(Dxy,grad2,tf.squeeze(var1).shape.__len__())\n",
    "def calculate_gradients(var1,var2):\n",
    "    # print(T.trainable_variables[k].shape)\n",
    "    with tf.GradientTape(persistent=True) as tape2: \n",
    "        with tf.GradientTape(persistent=True) as tape1: \n",
    "            loss1 = shared_loss(train_t,NN,T,g)\n",
    "        grad = tape1.gradient(loss1, var1)\n",
    "    Dxy = tape2.jacobian(grad,var2)\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape4: \n",
    "        with tf.GradientTape(persistent=True) as tape3: \n",
    "            loss2 = -shared_loss(train_t,NN,T,g)\n",
    "        grad2 = tape3.gradient(loss2, var2)\n",
    "    Dyx = tape4.jacobian(grad2,var1)\n",
    "    return Dyx,Dxy,grad,grad2\n",
    "    \n",
    "def apply_gradients(Dyx,Dxy,grad,grad2):\n",
    "    if contains1(var1):\n",
    "        # Dxy = tf.squeeze(Dxy)\n",
    "        n_params = np.max(var1.shape)\n",
    "        Identity = tf.eye(n_params)\n",
    "        grad = tf.reshape(grad, [n_params, 1])\n",
    "        grad2 = tf.reshape(grad2, [n_params, 1])\n",
    "        Dxy = tf.reshape(Dxy, [n_params, n_params])\n",
    "        Dyx = tf.reshape(Dyx, [n_params, n_params])\n",
    "    else:\n",
    "        try:\n",
    "            Identity = tf.eye(var1.shape)\n",
    "        except Exception:\n",
    "            Identity =  tf.eye(var1.shape[0])\n",
    "        \n",
    "    M1 = tf.linalg.inv(tf.eye(32) - mu**2 *  tf.tensordot(Dxy,Dyx,dim)) #32 ist statisch\n",
    "    M2  = grad - mu *  tf.tensordot(Dxy,grad2,dim)\n",
    "    \n",
    "    # M1 = tf.linalg.inv(Identity - mu**2 * Dxy @ Dyx)\n",
    "    # M2  = grad_f - mu * h_xy @ grad_g\n",
    "    # print(M1@M2.shape)\n",
    "    U = tf.reshape(tf.tensordot(M1,M2,dim),var1.shape)\n",
    "    var1.assign_sub(mu*U)\n",
    "def contains1(var):\n",
    "    for i in var.shape:\n",
    "        if i == 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# for k in range(0,T.trainable_variables.__len__()):\n",
    "#     var1 = NN.trainable_variables[k]\n",
    "#     var2 = T.trainable_variables[k]\n",
    "#     shape = tf.squeeze(var1).shape\n",
    "#     dim = tf.squeeze(var1).shape.__len__()\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "#     if var1.shape == [1]:\n",
    "#         Dyx,Dxy,grad,grad2 = calculate_gradients(var1,var2)\n",
    "#         M1 = 1/(1 - mu**2 *  Dxy[0] * Dyx[0])\n",
    "#         M2  = grad[0] - mu *  Dxy[0] * grad[0]\n",
    "#         var1.assign_sub(mu*M1 * M2)\n",
    "        \n",
    "#         Dyx,Dxy,grad,grad2 = calculate_gradients(var2,var1)\n",
    "#         M1 = 1/(1 - mu**2 *  Dxy[0] * Dyx[0])\n",
    "#         M2  = grad[0] - mu *  Dxy[0] * grad[0]\n",
    "#         var1.assign_sub(mu*M1 * M2)\n",
    "#         continue\n",
    "        \n",
    "#     apply_gradients(*calculate_gradients(var1,var2))\n",
    "#     apply_gradients(*calculate_gradients(var2,var1))\n",
    "        \n",
    "    #mu?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "for k in range(epochs): \n",
    "    print(k)\n",
    "    for k in range(0,T.trainable_variables.__len__()):\n",
    "        \n",
    "        var1 = NN.trainable_variables[k]\n",
    "        var2 = T.trainable_variables[k]\n",
    "        shape = tf.squeeze(var1).shape\n",
    "        dim = tf.squeeze(var1).shape.__len__()\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "        if var1.shape == [1]:\n",
    "            Dyx,Dxy,grad,grad2 = calculate_gradients(var1,var2)\n",
    "            M1 = 1/(1 - mu**2 *  Dxy[0] * Dyx[0])\n",
    "            M2  = grad[0] - mu *  Dxy[0] * grad[0]\n",
    "            var1.assign_sub(mu*M1 * M2)\n",
    "            \n",
    "            Dyx,Dxy,grad,grad2 = calculate_gradients(var2,var1)\n",
    "            M1 = 1/(1 - mu**2 *  Dxy[0] * Dyx[0])\n",
    "            M2  = grad[0] - mu *  Dxy[0] * grad[0]\n",
    "            var1.assign_sub(mu*M1 * M2)\n",
    "            continue\n",
    "            \n",
    "        apply_gradients(*calculate_gradients(var1,var2))\n",
    "        apply_gradients(*calculate_gradients(var2,var1))\n",
    "        loss1 = shared_loss(train_t,NN,T,g)\n",
    "        train_loss_record1.append(loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23089646"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "t = np.linspace(0,1,10**2)\n",
    "t = t.reshape((t.shape[0],1))\n",
    "\n",
    "def trapezoid(t, y):\n",
    "    return math_ops.reduce_sum(\n",
    "            math_ops.multiply( t[1:] - t[:-1],\n",
    "                              (y[:-1] + y[1:]) / 2.))\n",
    "            #  name='trapezoidal_integral_approx')\n",
    "np.sqrt ( trapezoid(t,(NN(t) - np.sin(t))**2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neu rechnen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cgd konvergiert nicht wir versuchen sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/google-deepmind/symplectic-gradient-adjustment/blob/master/Symplectic_Gradient_Adjustment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_divide_scalar(xs, y):\n",
    "  return [x / y for x in xs]\n",
    "\n",
    "\n",
    "def list_subtract(xs, ys):\n",
    "  return [x - y for (x, y) in zip(xs, ys)]\n",
    "\n",
    "\n",
    "def jacobian_vec(ys, xs, vs):\n",
    "  return tf.autodiff.ForwardAccumulator(\n",
    "      ys, xs, grad_xs=vs, stop_gradients=xs)\n",
    "\n",
    "def jacobian_transpose_vec(ys, xs, vs):\n",
    "  dydxs = tf.gradients(ys, xs, grad_ys=vs, stop_gradients=xs)\n",
    "  dydxs = [\n",
    "      tf.zeros_like(x) if dydx is None else dydx for x, dydx in zip(xs, dydxs)\n",
    "  ]\n",
    "  return dydxs\n",
    "\n",
    "\n",
    "def   _dot(x, y):\n",
    "  dot_list = []\n",
    "  for xx, yy in zip(x, y):\n",
    "    dot_list.append(tf.reduce_sum(xx * yy))\n",
    "  return tf.add_n(dot_list)\n",
    "\n",
    "\n",
    "class SymplecticOptimizer(tf.compat.v1.train.Optimizer):\n",
    "  \"\"\"Optimizer that corrects for rotational components in gradients.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate,\n",
    "               reg_params=1.,\n",
    "               use_signs=True,\n",
    "               use_locking=False,\n",
    "               name='symplectic_optimizer'):\n",
    "    super(SymplecticOptimizer, self).__init__(\n",
    "        use_locking=use_locking, name=name)\n",
    "    self._gd = tf.train.RMSPropOptimizer(learning_rate)\n",
    "    self._reg_params = reg_params\n",
    "    self._use_signs = use_signs\n",
    "\n",
    "  def compute_gradients(self,\n",
    "                        loss,\n",
    "                        var_list=None,\n",
    "                        gate_gradients=tf.compat.v1.train.Optimizer.GATE_OP,\n",
    "                        aggregation_method=None,\n",
    "                        colocate_gradients_with_ops=False,\n",
    "                        grad_loss=None):\n",
    "    return self._gd.compute_gradients(loss, var_list, gate_gradients,\n",
    "                                      aggregation_method,\n",
    "                                      colocate_gradients_with_ops, grad_loss)\n",
    "\n",
    "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
    "    grads, vars_ = zip(*grads_and_vars)\n",
    "    n = len(vars_)\n",
    "    h_v = jacobian_vec(grads, vars_, grads)\n",
    "    ht_v = jacobian_transpose_vec(grads, vars_, grads)\n",
    "    at_v = list_divide_scalar(list_subtract(ht_v, h_v), 2.)\n",
    "    if self._use_signs:\n",
    "      grad_dot_h = _dot(grads, ht_v)\n",
    "      at_v_dot_h = _dot(at_v, ht_v)\n",
    "      mult = grad_dot_h * at_v_dot_h\n",
    "      lambda_ = tf.sign(mult / n + 0.1) * self._reg_params\n",
    "    else:\n",
    "      lambda_ = self._reg_params\n",
    "    apply_vec = [(g + lambda_ * ag, x)\n",
    "                 for (g, ag, x) in zip(grads, at_v, vars_)\n",
    "                 if at_v is not None]\n",
    "    return self._gd.apply_gradients(apply_vec, global_step, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(epochs):\n",
    "    with tf.GradientTape(persistent=True) as tape: \n",
    "        loss = shared_loss(train_t,NN,T,g)\n",
    "        \n",
    "    grad1 = tape.gradient(loss, trainable_vars1)\n",
    "    grad2 = tape.gradient(loss, trainable_vars2)\n",
    "    \n",
    "    hess_xy = tape.gradient(grad1,trainable_vars2)\n",
    "    hess_yx = tape.gradient(grad2,trainable_vars1)\n",
    "    \n",
    "    \n",
    "    optimizer.apply_gradients(grad,hess_xy, trainable_vars)\n",
    "    train_loss_record1.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x197af111100>]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(1)\n",
    "axs.set_xlabel(\"t\")\n",
    "'train_loss_record1'\n",
    "axs.plot(train_loss_record1[:50],\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 77ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = NN.predict(train_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for _ in range(epochs):\n",
    "    with tf.GradientTape(persistent=True) as tape: \n",
    "        loss = shared_loss(train_t,NN,T,g)\n",
    "        \n",
    "    grad1 = tape.gradient(loss, trainable_vars1)\n",
    "    grad2 = tape.gradient(loss, trainable_vars2)\n",
    "    \n",
    "    hess_xy = tape.gradient(grad1,trainable_vars2)\n",
    "    hess_yx = tape.gradient(grad2,trainable_vars1)\n",
    "    \n",
    "    \n",
    "    optimizer.apply_gradients(grad,hess_xy, trainable_vars)\n",
    "    train_loss_record1.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "for k in var.shape:\n",
    "    print(k== None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = NN.trainable_variables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "print(tf.squeeze(var1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for var in trainable_vars1:\n",
    "    print(np.max(var.shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beispiel docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape(persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m t2: \n\u001b[0;32m     16\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape(persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m t1: \n\u001b[1;32m---> 17\u001b[0m       loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mshared_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m   g \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m.\u001b[39mgradient(loss1, trainable_vars1)\n\u001b[0;32m     20\u001b[0m h \u001b[38;5;241m=\u001b[39m t2\u001b[38;5;241m.\u001b[39mjacobian(g, trainable_vars2)\n",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36mshared_loss\u001b[1;34m(t, NN, T, g)\u001b[0m\n\u001b[0;32m     10\u001b[0m     dnn \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(u, t)\n\u001b[0;32m     11\u001b[0m     ddnn \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(dnn,t)\n\u001b[1;32m---> 12\u001b[0m ode_loss \u001b[38;5;241m=\u001b[39m  (a \u001b[38;5;241m*\u001b[39m ddnn \u001b[38;5;241m+\u001b[39m b \u001b[38;5;241m*\u001b[39m dnn \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m*\u001b[39m u \u001b[38;5;241m-\u001b[39m \u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     14\u001b[0m iv_loss \u001b[38;5;241m=\u001b[39m NN(t_0) \u001b[38;5;66;03m# we preceed with one intital value loss y_prime\u001b[39;00m\n\u001b[0;32m     15\u001b[0m square_loss \u001b[38;5;241m=\u001b[39m T(t) \u001b[38;5;241m*\u001b[39m( tf\u001b[38;5;241m.\u001b[39msquare(ode_loss) \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39msquare(iv_loss) )\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object is not callable"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal([7, 5])\n",
    "l1 = tf.keras.layers.Dense(8, activation=tf.nn.relu)\n",
    "l2 = tf.keras.layers.Dense(6, activation=tf.nn.relu)\n",
    "\n",
    "with tf.GradientTape() as t2:\n",
    "  with tf.GradientTape() as t1:\n",
    "    x = l1 (x)\n",
    "    x = l2 (x)\n",
    "    loss = tf.reduce_mean(x **2)\n",
    "\n",
    "  g = t1.gradient(loss, l1.kernel)\n",
    "\n",
    "h = t2.jacobian(g, l1.kernel)\n",
    "NN.trainable_variables[0]\n",
    "with tf.GradientTape(persistent=True) as t2: \n",
    "  with tf.GradientTape(persistent=True) as t1: \n",
    "      loss1 = shared_loss(train_t,NN,T,g)\n",
    "  g = t1.gradient(loss1, trainable_vars1)\n",
    "\n",
    "h = t2.jacobian(g, trainable_vars2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=40>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_params = tf.reduce_prod(layer1.kernel.shape)\n",
    "n_params\n",
    "# g_vec = tf.reshape(g, [n_params, 1])\n",
    "# h_mat = tf.reshape(h, [n_params, n_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.linalg.inv(  Dyx[0] @ Dxy[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig, axs = plt.subplots(1)\n",
    "# axs.set_xlabel(\"t\")\n",
    "# axs.set_ylabel(\"pred\")\n",
    "# axs.plot(train_loss_record1,\"b\")\n",
    "# axs.spines['top'].set_visible(False)\n",
    "# axs.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = shared_loss(train_t,NN,T,g)\n",
    "grad2 = tape.gradient(loss, trainable_vars2)\n",
    "# hess_yx = tape.gradient(grad2,trainable_vars1)\n",
    "grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C_GradientDescent(tf.Module):\n",
    "\n",
    "  def __init__(self, learning_rate=1e-3):\n",
    "    # Initialize parameters\n",
    "    self.learning_rate = learning_rate\n",
    "    self.title = f\"Gradient descent optimizer: learning rate={self.learning_rate}\"\n",
    "\n",
    "  def apply_gradients(self, grads,hess, vars):\n",
    "    # Update variables\n",
    "    for grad,hess, var in zip(grads,hess, vars):\n",
    "      print(type(var))\n",
    "      print(type(hess))\n",
    "      print(type(grads))\n",
    "      var.assign_sub(self.learning_rate*grad + self.learning_rate**2  * (hess * grad))\n",
    "# dy_dx = gg.gradient(y, x)\n",
    "loss = shared_loss(train_t,NN,T,g)\n",
    "grad2 = tape.gradient(loss, trainable_vars2)\n",
    "hess_yx = tape.gradient(grad2,trainable_vars1)\n",
    "\n",
    "optimizer = C_GradientDescent()\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for _ in range(epochs):\n",
    "    with tf.GradientTape(persistent=True) as tape: \n",
    "        loss = shared_loss(train_t,NN,T,g)\n",
    "        \n",
    "    grad1 = tape.gradient(loss, trainable_vars1)\n",
    "    grad2 = tape.gradient(loss, trainable_vars2)\n",
    "    \n",
    "    hess_xy = tape.gradient(grad1,trainable_vars2)\n",
    "    hess_yx = tape.gradient(grad2,trainable_vars1)\n",
    "    \n",
    "    \n",
    "    optimizer.apply_gradients(grad,hess_xy, trainable_vars)\n",
    "    train_loss_record1.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(32):\n",
    "    for j in range(32):\n",
    "        \n",
    "        A = tf.zeros((32,32))\n",
    "        for k in range(0,32):\n",
    "            tf.add(Dxy[i][k] @ Dyx[k][j],A)\n",
    "        # B[i][j] = A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "links\n",
    "https://openreview.net/pdf?id=HJechEHeLr\n",
    "https://www.tensorflow.org/guide/advanced_autodiff#hessian\n",
    "https://proceedings.mlr.press/v119/schaefer20a/schaefer20a.pdf\n",
    "https://f-t-s.github.io/projects/cgd/\n",
    "https://arxiv.org/pdf/2204.11144.pdf\n",
    "\n",
    "latex nn\n",
    "http://chebe163.caltech.edu/2018w/handouts/intro_to_latex.html#:~:text=The%20align%20environment%20in%20Latex,line%20at%20the%20alignment%20point.\n",
    "https://deeplearningmath.org/general-fully-connected-neural-networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
