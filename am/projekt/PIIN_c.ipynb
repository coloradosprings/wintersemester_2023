{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = 1,1,1,1\n",
    "NN = Sequential([\n",
    "    tf.keras.layers.Input((1,)),\n",
    "    tf.keras.layers.Dense(units = 32, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(units = 32, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(units = 1)\n",
    "])\n",
    "T = Sequential([\n",
    "    tf.keras.layers.Input((1,)),\n",
    "    tf.keras.layers.Dense(units = 32, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(units = 32, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(units = 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_vars1 = NN.trainable_variables\n",
    "trainable_vars2 = T.trainable_variables\n",
    "g = lambda x : np.cos(x)\n",
    "train_t = (np.array([0., 0.025, 0.475, 0.5, 0.525, 0.9, 0.95, 1., 1.05, 1.1, 1.4, 1.45, 1.5, 1.55, 1.6, 1.95, 2.])).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_loss(t,NN,T,g):\n",
    "    t = t.reshape(-1,1)\n",
    "    t = tf.constant(t, dtype = tf.float32)\n",
    "    t_0 = tf.zeros((1,1))\n",
    "    one = tf.ones((1,1))\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(t)\n",
    "        u = NN(t)\n",
    "        dnn = tape.gradient(u, t)\n",
    "        ddnn = tape.gradient(dnn,t)\n",
    "    ode_loss =  (a * ddnn + b * dnn + c * u - g(t))\n",
    "   \n",
    "    iv_loss = NN(t_0) # we preceed with one intital value loss y_prime\n",
    "    square_loss = T(t) *( tf.square(ode_loss) + tf.square(iv_loss) )\n",
    "\n",
    "    total_loss = tf.reduce_mean(square_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "competitive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    }
   ],
   "source": [
    "train_t = (np.array([0., 0.025, 0.475, 0.5, 0.525, 0.9, 0.95, 1., 1.05, 1.1, 1.4, 1.45, 1.5, 1.55, 1.6, 1.95, 2.])).reshape(-1, 1)\n",
    "train_loss_record1 = []\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "epochs = 300\n",
    "for _ in range(epochs):\n",
    "    with tf.GradientTape(persistent=True) as tape: \n",
    "        loss1 = shared_loss(train_t,NN,T,g)\n",
    "        loss2 =  -loss1\n",
    "    grad = tape.gradient(loss1, trainable_vars1)\n",
    "    optimizer.apply_gradients(zip(grad, trainable_vars1))\n",
    "    #discriminator\n",
    "    grad = tape.gradient(loss2, trainable_vars2)\n",
    "    optimizer.apply_gradients(zip(grad, trainable_vars2))\n",
    "    \n",
    "    train_loss_record1.append(loss1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002B7CD2E1430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002B7CD2E1430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002B7CD411790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002B7CD411790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 67ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = NN.predict(train_t)\n",
    "z_pred = T.predict(train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C_GradientDescent(tf.Module):\n",
    "\n",
    "  def __init__(self, learning_rate=1e-3):\n",
    "    # Initialize parameters\n",
    "    self.learning_rate = learning_rate\n",
    "    self.title = f\"Gradient descent optimizer: learning rate={self.learning_rate}\"\n",
    "\n",
    "  def apply_gradients(self, grads,hess, vars):\n",
    "    # Update variables\n",
    "    for grad,hess, var in zip(grads,hess, vars):\n",
    "      print(type(var))\n",
    "      print(type(hess))\n",
    "      print(type(grads))\n",
    "      var.assign_sub(self.learning_rate*grad + self.learning_rate**2  * (hess * grad))\n",
    "# dy_dx = gg.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x000002B7C0898EE0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x000002B7C0898EE0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x000002B7CD411A60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x000002B7CD411A60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "var1 = NN.trainable_variables[k]\n",
    "var2 = T.trainable_variables[k]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape2: \n",
    "    with tf.GradientTape(persistent=True) as tape1: \n",
    "        loss1 = shared_loss(train_t,NN,T,g)\n",
    "    grad = tape1.gradient(loss1, var1)\n",
    "Dxy = tape2.jacobian(grad,var2)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape4: \n",
    "    with tf.GradientTape(persistent=True) as tape3: \n",
    "        loss2 = -shared_loss(train_t,NN,T,g)\n",
    "    grad2 = tape3.gradient(loss2, var2)\n",
    "Dyx = tape4.jacobian(grad2,var1)\n",
    "\n",
    "\n",
    "    # grad2 = tape.gradient(loss2, trainable_vars2)\n",
    "    # Dyx = tape.gradient(grad2,trainable_vars1)\n",
    "\n",
    "# optimizer.apply_gradients(zip(grad, trainable_vars2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32, 32), dtype=float32, numpy=\n",
       "array([[[ 1.48003528e-06, -6.74346893e-07, -4.35832362e-06, ...,\n",
       "         -6.87902457e-07,  1.78457105e-06,  2.73893420e-06],\n",
       "        [ 1.97296984e-07, -1.56123306e-07, -5.85506882e-07, ...,\n",
       "         -5.72679184e-08,  3.09878601e-07,  3.34831299e-07],\n",
       "        [-2.23069492e-08,  4.16340612e-07,  7.23300673e-08, ...,\n",
       "         -2.01396261e-07, -4.67658111e-07,  1.58435626e-07],\n",
       "        ...,\n",
       "        [ 1.57482187e-08, -2.02076308e-07, -9.41862766e-08, ...,\n",
       "          9.30471558e-08,  2.32027929e-07, -3.69569264e-08],\n",
       "        [-2.19159210e-06,  5.30763941e-07,  4.65573885e-06, ...,\n",
       "          1.21259893e-06, -2.07354810e-06, -3.09521783e-06],\n",
       "        [-6.44952343e-06, -4.09700306e-07,  1.82010644e-05, ...,\n",
       "          4.72277225e-06, -4.11779592e-06, -1.30913304e-05]],\n",
       "\n",
       "       [[-4.84378745e-07,  5.57237627e-07,  1.74960667e-06, ...,\n",
       "          5.85318389e-08, -9.60098532e-07, -9.42032329e-07],\n",
       "        [-5.31573789e-08,  1.24472280e-07,  2.08342911e-07, ...,\n",
       "         -2.61617785e-08, -1.74598028e-07, -8.24142461e-08],\n",
       "        [-4.90440470e-08, -2.91069853e-07,  9.56444524e-08, ...,\n",
       "          1.84979584e-07,  2.82460036e-07, -2.15854925e-07],\n",
       "        ...,\n",
       "        [ 4.65966821e-08,  1.55854011e-07, -7.47629798e-08, ...,\n",
       "         -1.12320166e-07, -1.38027190e-07,  1.33670738e-07],\n",
       "        [ 1.35017103e-06, -1.32216201e-07, -3.05878552e-06, ...,\n",
       "         -8.53985455e-07,  1.07272342e-06,  2.13141766e-06],\n",
       "        [ 3.08732638e-06,  5.81671884e-08, -9.39442634e-06, ...,\n",
       "         -2.20776610e-06,  2.14402780e-06,  6.65018251e-06]],\n",
       "\n",
       "       [[-2.08970576e-07,  5.60014470e-08,  4.79392838e-07, ...,\n",
       "          1.13802400e-07, -2.04824943e-07, -3.16208968e-07],\n",
       "        [-2.82388868e-08,  1.11092291e-08,  6.45494680e-08, ...,\n",
       "          1.35239837e-08, -3.15128155e-08, -4.08006287e-08],\n",
       "        [ 3.79744591e-09, -3.21618359e-08, -2.26508234e-09, ...,\n",
       "          1.43629935e-08,  3.72861280e-08, -1.41617242e-08],\n",
       "        ...,\n",
       "        [-1.56745923e-08,  9.27149379e-09,  3.60272452e-08, ...,\n",
       "          5.89195004e-09, -2.08663469e-08, -2.12183338e-08],\n",
       "        [ 1.19967325e-07, -7.28904865e-08, -2.22789453e-07, ...,\n",
       "         -4.26126689e-08,  1.59962894e-07,  1.26123695e-07],\n",
       "        [ 5.81964173e-07, -8.50543813e-08, -1.34959896e-06, ...,\n",
       "         -3.54331888e-07,  4.94001711e-07,  9.25853783e-07]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-7.16722184e-08,  2.81400133e-07,  4.04641128e-07, ...,\n",
       "         -9.10891629e-08, -3.62826881e-07, -1.36290566e-07],\n",
       "        [-6.10032913e-10,  6.10324946e-08,  3.26678062e-08, ...,\n",
       "         -3.05583328e-08, -6.76854057e-08,  8.88540086e-09],\n",
       "        [-4.58020963e-08, -1.34136997e-07,  1.04082289e-07, ...,\n",
       "          1.01313738e-07,  1.13945468e-07, -1.42190714e-07],\n",
       "        ...,\n",
       "        [ 3.41457920e-08,  7.20668680e-08, -6.88872817e-08, ...,\n",
       "         -6.07109314e-08, -5.49533041e-08,  8.61592611e-08],\n",
       "        [ 5.12555744e-07, -3.55328389e-08, -1.18331468e-06, ...,\n",
       "         -3.32457944e-07,  3.92082598e-07,  8.31187208e-07],\n",
       "        [ 9.50565266e-07, -1.07073902e-07, -3.07588766e-06, ...,\n",
       "         -6.19651587e-07,  8.01957981e-07,  2.10661619e-06]],\n",
       "\n",
       "       [[ 2.08680581e-06, -9.29674911e-07, -5.69270742e-06, ...,\n",
       "         -9.68356403e-07,  2.47792923e-06,  3.57234831e-06],\n",
       "        [ 2.73588682e-07, -2.06973908e-07, -7.48710931e-07, ...,\n",
       "         -8.26205380e-08,  4.17238113e-07,  4.27178577e-07],\n",
       "        [ 8.70483063e-09,  5.59937405e-07, -4.49192612e-08, ...,\n",
       "         -2.98836540e-07, -6.00469434e-07,  3.12217537e-07],\n",
       "        ...,\n",
       "        [ 5.77176316e-08, -2.40634222e-07, -1.84424820e-07, ...,\n",
       "          8.46106971e-08,  3.02583572e-07,  8.83122198e-09],\n",
       "        [-2.50040807e-06,  9.41001588e-07,  5.04977015e-06, ...,\n",
       "          1.20112645e-06, -2.72055149e-06, -3.18842467e-06],\n",
       "        [-8.15427484e-06,  1.06129846e-07,  2.19607828e-05, ...,\n",
       "          5.61616753e-06, -5.84755026e-06, -1.55409489e-05]],\n",
       "\n",
       "       [[ 7.54233497e-07, -6.36386176e-07, -2.26142015e-06, ...,\n",
       "         -1.98941052e-07,  1.22837469e-06,  1.27580779e-06],\n",
       "        [ 8.78708022e-08, -1.38197990e-07, -2.71660156e-07, ...,\n",
       "          1.00195763e-08,  2.12868727e-07,  1.21905785e-07],\n",
       "        [ 5.82663517e-08,  3.32639530e-07, -1.40368854e-07, ...,\n",
       "         -2.13639481e-07, -3.20396708e-07,  2.67532698e-07],\n",
       "        ...,\n",
       "        [-1.89292049e-08, -1.59473487e-07,  1.85827993e-08, ...,\n",
       "          9.54760395e-08,  1.60781184e-07, -9.47383754e-08],\n",
       "        [-1.36804056e-06,  2.88631099e-07,  2.98083387e-06, ...,\n",
       "          7.81286531e-07, -1.25061911e-06, -2.00316322e-06],\n",
       "        [-3.63071649e-06,  2.35724201e-07,  1.02944705e-05, ...,\n",
       "          2.41667271e-06, -2.82571932e-06, -7.16813292e-06]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.matvec(Dxy, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains1(var):\n",
    "    for i in var.shape:\n",
    "        if i == 1:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_params = np.max(var1.shape)\n",
    "\n",
    "# grad = tf.reshape(grad, [n_params, 1])\n",
    "# grad2 = tf.reshape(grad2, [n_params, 1])\n",
    "# Dxy = tf.reshape(Dxy, [n_params, n_params])\n",
    "# Dyx = tf.reshape(Dxy, [n_params, n_params])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 10e-2\n",
    "dim = tf.squeeze(var1).shape.__len__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0,32):\n",
    "    grad_k = tf.reshape(grad[k], [n_params, 1])\n",
    "    Dxy[0][k] @ grad_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[0.00022872]]]], dtype=float32)>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tf.tensordot(Dxy,grad2,dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2  = grad - mu *  tf.linalg.matvec(Dxy,grad2,dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "M1 = tf.linalg.inv(tf.eye(32) - mu**2 *  tf.tensordot(Dxy,Dyx,dim))\n",
    "M2  = grad - mu *  tf.linalg.matvec(Dxy,grad2,dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'v1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241m.\u001b[39mRMSPropOptimizer()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'v1'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "train_loss_record1 = []\n",
    "epochs = 300\n",
    "for k in range(epochs):\n",
    "    with tf.GradientTape(persistent=True) as tape: \n",
    "        loss1 = shared_loss(train_t,NN,T,g)\n",
    "        loss2 =  -loss1\n",
    "    grad = tape.gradient(loss1, trainable_vars1)\n",
    "    optimizer.apply_gradients(zip(grad, trainable_vars1))\n",
    "    #discriminator\n",
    "    grad = tape.gradient(loss2, trainable_vars2)\n",
    "    optimizer.apply_gradients(zip(grad, trainable_vars2))\n",
    "    \n",
    "    train_loss_record1.append(loss1)\n",
    "    # print(\"k : \", k)\n",
    "    # print(shared_loss(train_t,NN,T,g))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.05258881>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.117624976>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16457541>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18817239>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1917912>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18365759>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17178226>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16123372>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15407896>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15049456>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14983782>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15129046>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15414008>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15785652>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16207987>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.166581>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17122142>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17592111>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18063584>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18534207>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19002779>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19468683>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1993162>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20391409>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20847929>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21301062>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21750695>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.22196698>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.22638935>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.23077258>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.235115>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.23941503>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.24367103>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.24788107>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.25204366>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.25615692>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2602191>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.26422855>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.26818368>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.27208257>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.27592382>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.27970573>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.28342673>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.28708535>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29068017>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29420954>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29767215>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.30106667>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.30439162>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.30764574>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.31082758>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3139361>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.31696987>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3199276>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.32280824>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.32561046>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.32833317>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3309752>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33353543>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3360126>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33840585>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34071392>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34293613>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34507108>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34711802>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34907588>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3509438>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35272098>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35440645>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35599944>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3574991>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35890478>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3602157>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36143127>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3625509>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36357385>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36449984>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36532813>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36605847>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3666904>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3672236>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3676579>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36799294>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36822864>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36836493>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3684016>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36833897>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36817697>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3679158>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36755562>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3670969>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36653993>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3658852>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36513323>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36428472>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3633402>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3623007>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.36116692>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3599399>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35862064>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35721025>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35571003>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35412118>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3524453>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.35068372>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3488381>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34691003>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34490132>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.342814>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.34064966>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33841062>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33609888>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3337165>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.33126608>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.32874954>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.32616946>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3235283>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.32082868>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.31807292>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.31526387>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.312404>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.30949634>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3065434>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.3035479>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.30051255>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2974404>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29433414>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.29119638>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2880301>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.284838>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.281623>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2783877>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.27513465>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2718668>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2685865>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.26529652>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.26199928>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.25869727>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.25539285>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2520883>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.24878614>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.24548823>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.24219687>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.23891404>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.23564167>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2323817>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2291359>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.22590601>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2226937>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21950035>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21632761>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21317679>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21004918>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20694612>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20386858>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20081776>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19779457>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1948>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1918348>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18889983>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18599588>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18312347>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18028319>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17747562>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1747011>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17196015>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16925319>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16658048>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16394216>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16133855>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15876989>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15623608>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1537375>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15127394>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14884563>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14645244>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14409427>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14177129>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13948306>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13722973>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.135011>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1328268>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13067682>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12856099>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12647896>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.124430515>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12241548>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12043349>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11848427>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11656753>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11468299>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11283033>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1110093>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10921941>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10746065>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10573274>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10403573>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10237021>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.100737564>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.099141814>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09759126>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09610675>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09473389>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.093579516>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09289186>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09324501>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09603824>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10445258>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12674037>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18058561>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.31620023>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.6197874>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.3692577>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.641863>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.1645365>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.7479606>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=6.785377>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.3835583>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.2429726>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0286782>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.6256196>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.40149486>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.31207445>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.26318038>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.23776108>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.22128105>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20990594>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20094869>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.19346419>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18686244>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18086004>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17530815>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.17009819>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16518866>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16052546>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15609662>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15186876>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14783867>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1439825>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14030035>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13677342>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13340366>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13017558>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12709287>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.124143235>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.121332586>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11865264>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1161136>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.113714606>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.111474335>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10940844>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.107553564>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.105965726>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10472358>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10398787>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10393831>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10502518>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.10767759>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.11318766>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.12258576>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13973393>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16689786>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.21604177>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2901162>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.42616916>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.6113251>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9500445>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2971934>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.8920654>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.106662>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.4972355>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.066397>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.8643891>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2829714>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.98251027>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.6741865>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.5140568>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.38612792>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.31540498>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.26154667>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2291652>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.20400757>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.18754971>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1742181>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.16483644>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15690164>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1510225>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14588201>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14199997>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1385533>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13604407>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13385728>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13254225>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13155282>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13157237>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13202956>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.1338857>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.13644503>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14121652>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.14721435>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_record1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29099db01f0>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1)\n",
    "axs.set_xlabel(\"t\")\n",
    "axs.set_ylabel(\"pred\")\n",
    "axs.plot(train_loss_record1,\"b\")\n",
    "axs.spines['top'].set_visible(False)\n",
    "axs.spines['right'].set_visible(False)\n",
    "axs.plot([0 for k in range(300)],\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- gradxf - mu * Dxyf * gradyf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x000002B7CC078D30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x000002B7CC078D30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape0: \n",
    "    with tf.GradientTape(persistent=True) as tape1: \n",
    "        loss2 = -shared_loss(train_t,NN,T,g)\n",
    "    grad1 = tape1.gradient(loss2, var2)\n",
    "Dyx = tape0.jacobian(grad1,var1)\n",
    "\n",
    "    \n",
    "    \n",
    "with tf.GradientTape(persistent=True) as tape4: \n",
    "    with tf.GradientTape(persistent=True) as tape3: \n",
    "        loss2 = shared_loss(train_t,NN,T,g)\n",
    "    with tf.GradientTape(persistent=True) as tape5: \n",
    "        loss3 = shared_loss(train_t,NN,T,g)\n",
    "    grad2 = tape3.gradient(loss2, var1)\n",
    "    grad3 = tape5.gradient(loss3, var2)\n",
    "    \n",
    "    \n",
    "Dxy_gradyf = tape2.gradient(grad,var2,output_gradients = grad1) #Hesse Vektor produkt\n",
    "# für Hesse*Hesse produkt spaltenweise Hess*gradient produkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in Dxy:\n",
    "#     print(k.shape)\n",
    "# grad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGA_iteration(mu = 1e-03):\n",
    "    for k in range(0,T.trainable_variables.__len__()):\n",
    "        # k = 4\n",
    "        \n",
    "        \n",
    "        var1 = NN.trainable_variables[k]\n",
    "        var2 = T.trainable_variables[k]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape2: \n",
    "            \n",
    "            with tf.GradientTape(persistent=True) as tape1: \n",
    "                loss = shared_loss(train_t,NN,T,g)\n",
    "            with tf.GradientTape(persistent=True) as tape0: \n",
    "                loss1 = shared_loss(train_t,NN,T,g)\n",
    "            grad = tape1.gradient(loss, var1)\n",
    "            grad1 = tape0.gradient(loss1, var2)\n",
    "            \n",
    "            \n",
    "        with tf.GradientTape(persistent=True) as tape4: \n",
    "            with tf.GradientTape(persistent=True) as tape3: \n",
    "                loss2 = shared_loss(train_t,NN,T,g)\n",
    "            with tf.GradientTape(persistent=True) as tape5: \n",
    "                loss3 = shared_loss(train_t,NN,T,g)\n",
    "            grad2 = tape3.gradient(loss2, var1)\n",
    "            grad3 = tape5.gradient(loss3, var2)\n",
    "            \n",
    "            \n",
    "        Dxy_gradyf = tape2.gradient(grad,var2,output_gradients = grad1) #Hesse Vektor produkt\n",
    "        # tf.print(Dxx_gradxf)\n",
    "\n",
    "        Dyx_gradxg = tape4.gradient(grad2,var1,output_gradients = grad2) #Hesse Vektor produkt\n",
    "        \n",
    "        var1.assign_sub(-mu * grad - mu *  Dxy_gradyf )\n",
    "        var2.assign_sub(-mu * grad2 - mu *  Dyx_gradxg )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k :  0\n",
      "tf.Tensor(5.979331, shape=(), dtype=float32)\n",
      "k :  1\n",
      "tf.Tensor(-1.8480353, shape=(), dtype=float32)\n",
      "k :  2\n",
      "tf.Tensor(-1.577327, shape=(), dtype=float32)\n",
      "k :  3\n",
      "tf.Tensor(3.9786193, shape=(), dtype=float32)\n",
      "k :  4\n",
      "tf.Tensor(-381671830000000.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "train_loss_record1 = []\n",
    "for k in range(epochs):\n",
    "    SGA_iteration()\n",
    "    print(\"k : \", k)\n",
    "    print(shared_loss(train_t,NN,T,g))\n",
    "    # train_loss_record1.append(shared_loss(train_t,NN,T,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=7.6302238>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.896523e+19>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_record1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OGDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/76093944/vjp-and-jvp-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         loss1 \u001b[38;5;241m=\u001b[39m shared_loss(train_t,NN,T,g)\n\u001b[0;32m      8\u001b[0m     grad \u001b[38;5;241m=\u001b[39m tape1\u001b[38;5;241m.\u001b[39mgradient(loss1, var1)\n\u001b[1;32m----> 9\u001b[0m Dxy_gradf \u001b[38;5;241m=\u001b[39m tape2\u001b[38;5;241m.\u001b[39mgradient(grad,var2,output_gradients \u001b[38;5;241m=\u001b[39m \u001b[43mgrad2\u001b[49m)\n\u001b[0;32m     10\u001b[0m Dxx_gradf \u001b[38;5;241m=\u001b[39m tape2\u001b[38;5;241m.\u001b[39mgradient(grad,var1,output_gradients \u001b[38;5;241m=\u001b[39m grad)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape(persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tape4: \n",
      "\u001b[1;31mNameError\u001b[0m: name 'grad2' is not defined"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "var1 = NN.trainable_variables[k]\n",
    "var2 = T.trainable_variables[k]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape2: \n",
    "    with tf.GradientTape(persistent=True) as tape1: \n",
    "        loss1 = shared_loss(train_t,NN,T,g)\n",
    "    grad = tape1.gradient(loss1, var1)\n",
    "Dxy_gradf = tape2.gradient(grad,var2,output_gradients = grad2)\n",
    "Dxx_gradf = tape2.gradient(grad,var1,output_gradients = grad)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape4: \n",
    "    with tf.GradientTape(persistent=True) as tape3: \n",
    "        loss2 = -shared_loss(train_t,NN,T,g)\n",
    "    grad2 = tape3.gradient(loss2, var2)\n",
    "    \n",
    "# Dyx = tape4.jacobian(grad2,var1)\n",
    "A_v = tape4.gradient(grad2,var1,output_gradients = grad2)\n",
    "\n",
    "var1 = grad - mu *  Dxy_gradf  + mu * tf.linalg.matvec(Dxy,grad)\n",
    "\n",
    "\n",
    "    # grad2 = tape.gradient(loss2, trainable_vars2)\n",
    "    # Dyx = tape.gradient(grad2,trainable_vars1)\n",
    "\n",
    "# optimizer.apply_gradients(zip(grad, trainable_vars2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OGDA_iteration(mu = 1e-03):\n",
    "    for k in range(0,T.trainable_variables.__len__()):\n",
    "        # k = 4\n",
    "        \n",
    "        \n",
    "        var1 = NN.trainable_variables[k]\n",
    "        var2 = T.trainable_variables[k]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape2: \n",
    "            \n",
    "            with tf.GradientTape(persistent=True) as tape1: \n",
    "                loss1 = shared_loss(train_t,NN,T,g)\n",
    "            with tf.GradientTape(persistent=True) as tape0: \n",
    "                loss = shared_loss(train_t,NN,T,g)\n",
    "            grad = tape1.gradient(loss1, var1)\n",
    "            grad0 = tape0.gradient(loss, var2)\n",
    "            \n",
    "            \n",
    "        with tf.GradientTape(persistent=True) as tape4: \n",
    "            with tf.GradientTape(persistent=True) as tape3: \n",
    "                loss2 = -shared_loss(train_t,NN,T,g)\n",
    "            with tf.GradientTape(persistent=True) as tape5: \n",
    "                loss3 = -shared_loss(train_t,NN,T,g)\n",
    "            grad2 = tape3.gradient(loss2, var2)\n",
    "            grad3 = tape5.gradient(loss3, var1)\n",
    "            \n",
    "        Dxy_gradyf = tape2.gradient(grad,var2,output_gradients = grad0) #Hesse Vektor produkt\n",
    "        # tf.print(Dxx_gradxf)\n",
    "        Dxx_gradxf = tape2.gradient(grad,var1,output_gradients = grad) # HEsse Vektor Product#noch zu überprüfen\n",
    "\n",
    "        # Dyy_gradyg = tape4.gradient(grad2,var2,output_gradients = grad2) # HEsse Vektor Product#noch zu überprüfen\n",
    "\n",
    "        Dyx_gradxg = tape4.gradient(grad2,var1,output_gradients = grad3) #Hesse Vektor produkt\n",
    "        \n",
    "        print\n",
    "        #berechne Dyy_gradyg mit forward \n",
    "        with tf.GradientTape() as grad_tape:\n",
    "            with  tf.autodiff.ForwardAccumulator (\n",
    "                var2, grad2) as acc:\n",
    "                loss = -shared_loss(train_t,NN,T,g)\n",
    "        Dyy_gradyg = grad_tape.gradient(acc.jvp(loss), var2)\n",
    "        # tf.print(Dyy_gradyg)\n",
    "        # tf.print(var2)\n",
    "\n",
    "\n",
    "            # Dyxg_gradg = tape4.gradient(grad2,var1,output_gradients = grad2)\n",
    "            # Dyy_gradxg = tape4\n",
    "\n",
    "            # var1 = grad - mu *  Dxy_gradf  + mu * Dxx_gradf\n",
    "            # var2 = grad2 - mu * D\n",
    "            #versuche es mit forward acc\n",
    "        var1.assign_sub(-grad - mu *  Dxy_gradyf  + mu * Dxx_gradxf)\n",
    "        var2.assign_sub(-grad2 - mu *  Dyx_gradxg  + mu * Dyy_gradyg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGA_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop sgda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k :  0\n",
      "k :  1\n",
      "k :  2\n",
      "k :  3\n",
      "k :  4\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "train_loss_record1 = []\n",
    "for k in range(epochs):\n",
    "    SGA_iteration()\n",
    "    print(\"k : \", k)\n",
    "    train_loss_record1.append(shared_loss(train_t,NN,T,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=14752.608>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_record1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward\n",
    "\n",
    "with tf.autodiff.ForwardAccumulator (\n",
    "    var2, grad2) as acc:\n",
    "    with tf.GradientTape() as grad_tape:\n",
    "        loss = shared_loss(train_t,NN,T,g)\n",
    "grad1 = grad_tape.gradient(loss, var2)\n",
    "Dyy = acc.jvp(grad1)\n",
    "grad1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as grad_tape:\n",
    "# grad_tape.watch(h1.trainable_variables)\n",
    "    # loss = -shared_loss(train_t,NN,T,g)\n",
    "    with  tf.autodiff.ForwardAccumulator (\n",
    "        var2, grad2) as acc:\n",
    "        loss = -shared_loss(train_t,NN,T,g)\n",
    "\n",
    "U = grad_tape.gradient(acc.jvp(loss), var2)\n",
    "#forward wirft keinen None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "var2 = T.trainable_variables[5]\n",
    "with tf.GradientTape(persistent=True) as tape4: \n",
    "    tape4.watch(var2)\n",
    "    with tf.GradientTape(persistent=True) as tape3: \n",
    "        loss = -shared_loss(train_t,NN,T,g)\n",
    "    grad2 = tape3.gradient(loss, var2)\n",
    "# print(type(grad2))\n",
    "Z = tape4.gradient(grad2,var2,output_gradients = grad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00, -1.4901161e-08,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.4901161e-08,\n",
       "       -5.8207661e-11,  0.0000000e+00, -1.4901161e-08,  0.0000000e+00,\n",
       "       -1.1641532e-10, -1.4901161e-08, -3.7252903e-09,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  3.7252903e-09,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        1.8626451e-09,  0.0000000e+00,  0.0000000e+00, -7.4505806e-09],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U - Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(t):\n",
    "    return np.cos(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "[-0.198037535 0.149052531 0.0767920911 ... 0.00750894845 0.0101512596 -0.00354607403]\n"
     ]
    }
   ],
   "source": [
    "var2 = NN.trainable_variables[3]\n",
    "with tf.GradientTape(persistent=True) as tape4: \n",
    "    with tf.GradientTape(persistent=True) as tape3: \n",
    "        loss = -shared_loss(train_t,NN,T,g)\n",
    "    grad2 = tape3.gradient(loss, var2)\n",
    "print(type(grad2))\n",
    "tf.print(tape4.gradient(grad2,var2))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setze es zu null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[-4.6110000e-07,  3.7966242e-05, -4.1607189e-05, ...,\n",
       "         2.6117861e-05,  1.8781768e-05, -2.6791133e-05],\n",
       "       [ 1.8468677e-07, -1.7177117e-05,  1.8871104e-05, ...,\n",
       "        -1.1814228e-05, -8.4978428e-06,  1.2170377e-05],\n",
       "       [ 1.7577227e-06, -8.4219442e-05,  9.0861446e-05, ...,\n",
       "        -5.8007514e-05, -4.1651398e-05,  5.7917787e-05],\n",
       "       ...,\n",
       "       [ 8.2121375e-07, -5.6645764e-05,  6.1817969e-05, ...,\n",
       "        -3.8980816e-05, -2.8020357e-05,  3.9698290e-05],\n",
       "       [ 1.1281536e-08, -1.0837645e-06,  1.1913679e-06, ...,\n",
       "        -7.4536513e-07, -5.3616446e-07,  7.6863489e-07],\n",
       "       [-9.2735581e-07,  6.0929073e-05, -6.6406414e-05, ...,\n",
       "         4.1932632e-05,  3.0138432e-05, -4.2609481e-05]], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1.assign_sub(grad - mu *  Dxy_gradyf  + mu * Dxx_gradxf)\n",
    "var2.assign_sub(grad - mu *  Dyx_gradxg  + mu * Dyy_gradyg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(32,) dtype=float32, numpy=\n",
       "array([-0.01087814, -0.00333859,  0.00283779,  0.00824017, -0.00798694,\n",
       "        0.00863228, -0.01211496,  0.01969597,  0.00918963,  0.00796708,\n",
       "       -0.00209234, -0.00804568,  0.01245896,  0.00431448,  0.00535099,\n",
       "        0.01060803,  0.01284291, -0.01190958, -0.0016384 , -0.00893993,\n",
       "       -0.01216437, -0.02015602, -0.00454608, -0.00563179, -0.01800698,\n",
       "        0.00070412, -0.00197394, -0.0015933 , -0.00205856,  0.00761206,\n",
       "        0.02295514,  0.00234789], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1.assign_sub(grad - mu *  Dxy_gradyf  + mu * Dxx_gradxf)\n",
    "var2.assign_sub(grad - mu *  Dyx_gradxg  + mu * Dyy_gradyg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as inner_tape:\n",
    "      logits = h1(images, training=True)\n",
    "      loss = tf.compat.v1.losses.softmax_cross_entropy(\n",
    "          logits=logits, onehot_labels=labels)\n",
    "    grads = inner_tape.gradient(loss, h1.trainable_variables)\n",
    "  return outer_tape.gradient(\n",
    "      grads, h1.trainable_variables, output_gradients=vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape: \n",
    "    grads = tape.gradient(grad,var2,output_gradients = grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad - mu *  tf.linalg.matvec(Dxy,grad2) + mu * tf.linalg.matvec(Dxy,grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[ 3.1644796e-04, -2.6273359e-05,  1.4123552e-04, ...,\n",
       "         5.2533165e-04, -8.4452091e-05, -2.9865134e-04],\n",
       "       [ 8.7899028e-04, -7.0649476e-05,  3.8539164e-04, ...,\n",
       "         1.4516520e-03, -2.3058898e-04, -8.3000376e-04],\n",
       "       [-8.9740520e-04,  7.2051494e-05, -3.9285363e-04, ...,\n",
       "        -1.4815319e-03,  2.3510943e-04,  8.4739702e-04],\n",
       "       ...,\n",
       "       [-5.9449417e-04,  4.8763224e-05, -2.6404811e-04, ...,\n",
       "        -9.8534022e-04,  1.5786031e-04,  5.6118850e-04],\n",
       "       [ 6.0653838e-04, -4.9715196e-05,  2.6931131e-04, ...,\n",
       "         1.0052011e-03, -1.6100633e-04, -5.7256542e-04],\n",
       "       [-1.0105843e-03,  8.2312232e-05, -4.3471943e-04, ...,\n",
       "        -1.6639642e-03,  2.6158770e-04,  9.5373206e-04]], dtype=float32)>"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.tensordot(Dxy,grad,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def jacobian_transpose_vec(ys, xs, vs):\n",
    "  dydxs = tf.gradients(ys, xs, grad_ys=vs, stop_gradients=xs)\n",
    "  dydxs = [\n",
    "      tf.zeros_like(x) if dydx is None else dydx for x, dydx in zip(xs, dydxs)\n",
    "  ]\n",
    "  return dydxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fall n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.0000044], dtype=float32)>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = 1/(1 - mu**2 *  Dxy[0] * Dyx[0])\n",
    "M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[0.01608469]]]], dtype=float32)>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(1,) dtype=float32, numpy=array([0.06397489], dtype=float32)>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = 1/(1 - mu**2 *  Dxy[0] * Dyx[0])\n",
    "M2  = grad[0] - mu *  Dxy[0] * grad[0]\n",
    "var1.assign_sub(mu*M1 * M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1.shape == [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sgda git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'Optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     dot_list\u001b[38;5;241m.\u001b[39mappend(tf\u001b[38;5;241m.\u001b[39mreduce_sum(xx \u001b[38;5;241m*\u001b[39m yy))\n\u001b[0;32m     33\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39madd_n(dot_list)\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSymplecticOptimizer\u001b[39;00m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOptimizer\u001b[49m):\n\u001b[0;32m     37\u001b[0m   \u001b[38;5;124;03m\"\"\"Optimizer that corrects for rotational components in gradients.\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     40\u001b[0m                learning_rate,\n\u001b[0;32m     41\u001b[0m                reg_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m,\n\u001b[0;32m     42\u001b[0m                use_signs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     43\u001b[0m                use_locking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m                name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymplectic_optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'"
     ]
    }
   ],
   "source": [
    "#@title Defining the SGA Optimiser\n",
    "def jacobian_transpose_vec(ys, xs, vs):\n",
    "  dydxs = tf.gradients(ys, xs, grad_ys=vs, stop_gradients=xs)\n",
    "  dydxs = [\n",
    "      tf.zeros_like(x) if dydx is None else dydx for x, dydx in zip(xs, dydxs)\n",
    "  ]\n",
    "  return dydxs\n",
    "def list_divide_scalar(xs, y):\n",
    "  return [x / y for x in xs]\n",
    "\n",
    "\n",
    "def list_subtract(xs, ys):\n",
    "  return [x - y for (x, y) in zip(xs, ys)]\n",
    "\n",
    "\n",
    "#\n",
    "# def jacobian_vec(ys, xs, vs):\n",
    "#   return kfac.utils.fwd_gradients(\n",
    "#       ys, xs, grad_xs=vs, stop_gradients=xs)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "def jacobian_vec(ys, xs, vs):\n",
    "  return tf.autodiff.ForwardAccumulator(\n",
    "      ys, xs, grad_xs=vs, stop_gradients=xs)\n",
    "\n",
    "\n",
    "def _dot(x, y):\n",
    "  dot_list = []\n",
    "  for xx, yy in zip(x, y):\n",
    "    dot_list.append(tf.reduce_sum(xx * yy))\n",
    "  return tf.add_n(dot_list)\n",
    "\n",
    "\n",
    "class SymplecticOptimizer(tf.train.Optimizer):\n",
    "  \"\"\"Optimizer that corrects for rotational components in gradients.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate,\n",
    "               reg_params=1.,\n",
    "               use_signs=True,\n",
    "               use_locking=False,\n",
    "               name='symplectic_optimizer'):\n",
    "    super(SymplecticOptimizer, self).__init__(\n",
    "        use_locking=use_locking, name=name)\n",
    "    self._gd = tf.train.RMSPropOptimizer(learning_rate)\n",
    "    self._reg_params = reg_params\n",
    "    self._use_signs = use_signs\n",
    "\n",
    "  def compute_gradients(self,\n",
    "                        loss,\n",
    "                        var_list=None,\n",
    "                        gate_gradients=tf.train.Optimizer.GATE_OP,\n",
    "                        aggregation_method=None,\n",
    "                        colocate_gradients_with_ops=False,\n",
    "                        grad_loss=None):\n",
    "    return self._gd.compute_gradients(loss, var_list, gate_gradients,\n",
    "                                      aggregation_method,\n",
    "                                      colocate_gradients_with_ops, grad_loss)\n",
    "\n",
    "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
    "    grads, vars_ = zip(*grads_and_vars)\n",
    "    n = len(vars_)\n",
    "    h_v = jacobian_vec(grads, vars_, grads)\n",
    "    ht_v = jacobian_transpose_vec(grads, vars_, grads)\n",
    "    at_v = list_divide_scalar(list_subtract(ht_v, h_v), 2.)\n",
    "    if self._use_signs:\n",
    "      grad_dot_h = _dot(grads, ht_v)\n",
    "      at_v_dot_h = _dot(at_v, ht_v)\n",
    "      mult = grad_dot_h * at_v_dot_h\n",
    "      lambda_ = tf.sign(mult / n + 0.1) * self._reg_params\n",
    "    else:\n",
    "      lambda_ = self._reg_params\n",
    "    apply_vec = [(g + lambda_ * ag, x)\n",
    "                 for (g, ag, x) in zip(grads, at_v, vars_)\n",
    "                 if at_v is not None]\n",
    "    return self._gd.apply_gradients(apply_vec, global_step, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tf.tensordot(Dxy,grad2,tf.squeeze(var1).shape.__len__())\n",
    "def calculate_gradients(var1,var2):\n",
    "    # print(T.trainable_variables[k].shape)\n",
    "    with tf.GradientTape(persistent=True) as tape2: \n",
    "        with tf.GradientTape(persistent=True) as tape1: \n",
    "            loss1 = shared_loss(train_t,NN,T,g)\n",
    "        grad = tape1.gradient(loss1, var1)\n",
    "    Dxy = tape2.jacobian(grad,var2)\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape4: \n",
    "        with tf.GradientTape(persistent=True) as tape3: \n",
    "            loss2 = -shared_loss(train_t,NN,T,g)\n",
    "        grad2 = tape3.gradient(loss2, var2)\n",
    "    Dyx = tape4.jacobian(grad2,var1)\n",
    "    return Dyx,Dxy,grad,grad2\n",
    "    \n",
    "def apply_gradients(Dyx,Dxy,grad,grad2):\n",
    "    if contains1(var1):\n",
    "        # Dxy = tf.squeeze(Dxy)\n",
    "        n_params = np.max(var1.shape)\n",
    "        Identity = tf.eye(n_params)\n",
    "        grad = tf.reshape(grad, [n_params, 1])\n",
    "        grad2 = tf.reshape(grad2, [n_params, 1])\n",
    "        Dxy = tf.reshape(Dxy, [n_params, n_params])\n",
    "        Dyx = tf.reshape(Dyx, [n_params, n_params])\n",
    "    else:\n",
    "        try:\n",
    "            Identity = tf.eye(var1.shape)\n",
    "        except Exception:\n",
    "            Identity =  tf.eye(var1.shape[0])\n",
    "        \n",
    "    M1 = tf.linalg.inv(tf.eye(32) - mu**2 *  tf.tensordot(Dxy,Dyx,dim)) #32 ist statisch\n",
    "    M2  = grad - mu *  tf.tensordot(Dxy,grad2,dim)\n",
    "    \n",
    "    # M1 = tf.linalg.inv(Identity - mu**2 * Dxy @ Dyx)\n",
    "    # M2  = grad_f - mu * h_xy @ grad_g\n",
    "    # print(M1@M2.shape)\n",
    "    U = tf.reshape(tf.tensordot(M1,M2,dim),var1.shape)\n",
    "    var1.assign_sub(mu*U)\n",
    "def contains1(var):\n",
    "    for i in var.shape:\n",
    "        if i == 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# for k in range(0,T.trainable_variables.__len__()):\n",
    "#     var1 = NN.trainable_variables[k]\n",
    "#     var2 = T.trainable_variables[k]\n",
    "#     shape = tf.squeeze(var1).shape\n",
    "#     dim = tf.squeeze(var1).shape.__len__()\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "#     if var1.shape == [1]:\n",
    "#         Dyx,Dxy,grad,grad2 = calculate_gradients(var1,var2)\n",
    "#         M1 = 1/(1 - mu**2 *  Dxy[0] * Dyx[0])\n",
    "#         M2  = grad[0] - mu *  Dxy[0] * grad[0]\n",
    "#         var1.assign_sub(mu*M1 * M2)\n",
    "        \n",
    "#         Dyx,Dxy,grad,grad2 = calculate_gradients(var2,var1)\n",
    "#         M1 = 1/(1 - mu**2 *  Dxy[0] * Dyx[0])\n",
    "#         M2  = grad[0] - mu *  Dxy[0] * grad[0]\n",
    "#         var1.assign_sub(mu*M1 * M2)\n",
    "#         continue\n",
    "        \n",
    "#     apply_gradients(*calculate_gradients(var1,var2))\n",
    "#     apply_gradients(*calculate_gradients(var2,var1))\n",
    "        \n",
    "    #mu?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "for k in range(epochs): \n",
    "    print(k)\n",
    "    for k in range(0,T.trainable_variables.__len__()):\n",
    "        \n",
    "        var1 = NN.trainable_variables[k]\n",
    "        var2 = T.trainable_variables[k]\n",
    "        shape = tf.squeeze(var1).shape\n",
    "        dim = tf.squeeze(var1).shape.__len__()\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "        if var1.shape == [1]:\n",
    "            Dyx,Dxy,grad,grad2 = calculate_gradients(var1,var2)\n",
    "            M1 = 1/(1 - mu**2 *  Dxy[0] * Dyx[0])\n",
    "            M2  = grad[0] - mu *  Dxy[0] * grad[0]\n",
    "            var1.assign_sub(mu*M1 * M2)\n",
    "            \n",
    "            Dyx,Dxy,grad,grad2 = calculate_gradients(var2,var1)\n",
    "            M1 = 1/(1 - mu**2 *  Dxy[0] * Dyx[0])\n",
    "            M2  = grad[0] - mu *  Dxy[0] * grad[0]\n",
    "            var1.assign_sub(mu*M1 * M2)\n",
    "            continue\n",
    "            \n",
    "        apply_gradients(*calculate_gradients(var1,var2))\n",
    "        apply_gradients(*calculate_gradients(var2,var1))\n",
    "        loss1 = shared_loss(train_t,NN,T,g)\n",
    "        train_loss_record1.append(loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23089646"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "t = np.linspace(0,1,10**2)\n",
    "t = t.reshape((t.shape[0],1))\n",
    "\n",
    "def trapezoid(t, y):\n",
    "    return math_ops.reduce_sum(\n",
    "            math_ops.multiply( t[1:] - t[:-1],\n",
    "                              (y[:-1] + y[1:]) / 2.))\n",
    "            #  name='trapezoidal_integral_approx')\n",
    "np.sqrt ( trapezoid(t,(NN(t) - np.sin(t))**2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neu rechnen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cgd konvergiert nicht wir versuchen sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/google-deepmind/symplectic-gradient-adjustment/blob/master/Symplectic_Gradient_Adjustment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_divide_scalar(xs, y):\n",
    "  return [x / y for x in xs]\n",
    "\n",
    "\n",
    "def list_subtract(xs, ys):\n",
    "  return [x - y for (x, y) in zip(xs, ys)]\n",
    "\n",
    "\n",
    "def jacobian_vec(ys, xs, vs):\n",
    "  return tf.autodiff.ForwardAccumulator(\n",
    "      ys, xs, grad_xs=vs, stop_gradients=xs)\n",
    "\n",
    "def jacobian_transpose_vec(ys, xs, vs):\n",
    "  dydxs = tf.gradients(ys, xs, grad_ys=vs, stop_gradients=xs)\n",
    "  dydxs = [\n",
    "      tf.zeros_like(x) if dydx is None else dydx for x, dydx in zip(xs, dydxs)\n",
    "  ]\n",
    "  return dydxs\n",
    "\n",
    "\n",
    "def   _dot(x, y):\n",
    "  dot_list = []\n",
    "  for xx, yy in zip(x, y):\n",
    "    dot_list.append(tf.reduce_sum(xx * yy))\n",
    "  return tf.add_n(dot_list)\n",
    "\n",
    "\n",
    "class SymplecticOptimizer(tf.compat.v1.train.Optimizer):\n",
    "  \"\"\"Optimizer that corrects for rotational components in gradients.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate,\n",
    "               reg_params=1.,\n",
    "               use_signs=True,\n",
    "               use_locking=False,\n",
    "               name='symplectic_optimizer'):\n",
    "    super(SymplecticOptimizer, self).__init__(\n",
    "        use_locking=use_locking, name=name)\n",
    "    self._gd = tf.train.RMSPropOptimizer(learning_rate)\n",
    "    self._reg_params = reg_params\n",
    "    self._use_signs = use_signs\n",
    "\n",
    "  def compute_gradients(self,\n",
    "                        loss,\n",
    "                        var_list=None,\n",
    "                        gate_gradients=tf.compat.v1.train.Optimizer.GATE_OP,\n",
    "                        aggregation_method=None,\n",
    "                        colocate_gradients_with_ops=False,\n",
    "                        grad_loss=None):\n",
    "    return self._gd.compute_gradients(loss, var_list, gate_gradients,\n",
    "                                      aggregation_method,\n",
    "                                      colocate_gradients_with_ops, grad_loss)\n",
    "\n",
    "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
    "    grads, vars_ = zip(*grads_and_vars)\n",
    "    n = len(vars_)\n",
    "    h_v = jacobian_vec(grads, vars_, grads)\n",
    "    ht_v = jacobian_transpose_vec(grads, vars_, grads)\n",
    "    at_v = list_divide_scalar(list_subtract(ht_v, h_v), 2.)\n",
    "    if self._use_signs:\n",
    "      grad_dot_h = _dot(grads, ht_v)\n",
    "      at_v_dot_h = _dot(at_v, ht_v)\n",
    "      mult = grad_dot_h * at_v_dot_h\n",
    "      lambda_ = tf.sign(mult / n + 0.1) * self._reg_params\n",
    "    else:\n",
    "      lambda_ = self._reg_params\n",
    "    apply_vec = [(g + lambda_ * ag, x)\n",
    "                 for (g, ag, x) in zip(grads, at_v, vars_)\n",
    "                 if at_v is not None]\n",
    "    return self._gd.apply_gradients(apply_vec, global_step, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(epochs):\n",
    "    with tf.GradientTape(persistent=True) as tape: \n",
    "        loss = shared_loss(train_t,NN,T,g)\n",
    "        \n",
    "    grad1 = tape.gradient(loss, trainable_vars1)\n",
    "    grad2 = tape.gradient(loss, trainable_vars2)\n",
    "    \n",
    "    hess_xy = tape.gradient(grad1,trainable_vars2)\n",
    "    hess_yx = tape.gradient(grad2,trainable_vars1)\n",
    "    \n",
    "    \n",
    "    optimizer.apply_gradients(grad,hess_xy, trainable_vars)\n",
    "    train_loss_record1.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x197af111100>]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAefUlEQVR4nO3deZRU1bXH8e9OI8YYFaOgiWDUiAMqonQQBQdINGA06DIqzmOQKAomSojvaXBAccKRQTSImhjEgSGIAyEICCpdnTAICHYQQ9Nit5KAAkpj7/fHKZ4lFnKb7uZW3fp91mLRdatu9b4ri1+O+557jrk7IiKSXN+KuwAREWlYCnoRkYRT0IuIJJyCXkQk4RT0IiIJ1yjuArLZfffdfZ999om7DBGRvFFaWvqRuzfN9l5OBv0+++xDKpWKuwwRkbxhZu9v7j21bkREEk5BLyKScAp6EZGEU9CLiCRcpKA3sy5mtsjMysysX5b3u5nZXDObbWYpM+sY9VwREWlYWwx6MysCBgNdgVbAOWbWapOPTQYOd/c2wKXAY7U4V0REGlCUEX07oMzdl7j7emAU0C3zA+7+qX+5DOaOgEc9V0REGlaUoN8LWJbxujx97CvM7HQzewd4kTCqj3xu+vwe6bZPqqqqKkrtIiKJ4A6vvgp33dUw3x8l6C3Lsa8tYu/uY9z9IOA04NbanJs+f7i7F7t7cdOmWR/uEhFJFHd4+WU45hj42c9g6FBYt67+f0+UoC8HWmS8bg5UbO7D7j4N+JGZ7V7bc0VECoE7vPgitG8PXbtCRQUMGwbvvAM77FD/vy9K0JcALc1sXzNrDHQHxmd+wMz2NzNL/3wk0Bj4OMq5IiKFwh0mTIB27eCUU6CyEoYPh3ffhSuugO23b5jfu8W1btx9g5n1Al4BioAR7j7fzHqm3x8GnAFcaGbVwDrg7PTN2aznNsyliIjkJneYOBH694dUCvbdFx57DC68ELbbruF/v+XinrHFxcWuRc1EJN9t7MH37w+zZsE++8CNN8IFF9R/wJtZqbsXZ3tPT8aKiNSzjbNojj4aTj4ZPvwQHn0UFi+GSy/dNqP4TAp6EZF6NGUKHHtsmEXzwQfwyCMh4C+/fNsH/EYKehGRejBjBnTuHP4sXQpDhoSA79EDGjeOtzYFvYhIHZSUQJcu0LEjLFgA998PZWXw61833Cya2lLQi4hshblzoVu3MFUylQpPtS5ZAr17w7e/HXd1X5WTWwmKiOSqxYvhD3+AZ56BnXeGW28N4b7TTnFXtnkKehGRCP79b7jlFhg5MrRk+vWD66+HXXeNu7ItU9CLiHyDykoYMCAsUQDQqxf8/vewxx7x1lUbCnoRkSxWrYK77w43Vz/7DC65BG66CVq02OKpOUdBLyKSYe1aePhhGDgQ/vMfOOus0Ic/4IC4K9t6mnUjIgJUV4eHm/bfH373OzjqKCgtDTdd8znkQUEvIgXOHZ59Fg45BHr2DOvRvPYavPQSHHlk3NXVDwW9iBSsyZPDPPizzgpPr44bF55wPf74uCurXwp6ESk4//gHnHQS/PSnYcGxxx+HOXPgF78Ay7YvXp5T0ItIwXjvPTj3XGjbNvTfBw0KD0BdfDEUFcVdXcPRrBsRSbyPPoLbbgsLjTVqBDfcAH37wi67xF3ZtqGgF5HEWrs2zIO/80749NOwFnz//rDXXnFXtm0p6EUkcb74Ap54IuzmVFEReu933AGtWsVdWTzUoxeRxHAP0yLbtIHLLoO994Zp08JsmkINeVDQi0hC/POfcOKJYeu+detg9GiYOTPs9lToFPQikteWLYMLLwwzaWbPhgceCBuAnHlmMqdKbg316EUkL61eHdajue++0LLp2zcsHdykSdyV5R4FvYjklQ0b4LHHwuYflZVw3nlhGeEf/jDuynKXgl5E8oI7TJwYNvtYuDD03idMgB//OO7Kcp969CKS8+bNC0sWnHJKGNGPGQNTpyrko1LQi0jOWrECevQI0yVLS8PDT2+/DaedphuttaHWjYjknHXrQqjffnvY3emaa8LDT9/7XtyV5ScFvYjkDPcw//13v4P334du3eCuu/J/44+4qXUjIjmhpAQ6doTu3cMUycmTYexYhXx9UNCLSKwqKuCii8IGIGVlYepkaSl07hx3ZckRKejNrIuZLTKzMjPrl+X988xsbvrPTDM7POO9pWY2z8xmm1mqPosXkfy1bl2Y/37AATBqVHjY6d13wxo1SV4bPg5b7NGbWREwGDgRKAdKzGy8uy/I+Nh7wPHu/h8z6woMB47KeL+Tu39Uj3WLSJ5yh+eeC/Ph338fzjgj9OH32y/uypIryoi+HVDm7kvcfT0wCuiW+QF3n+nu/0m/fBNoXr9likgSzJ4NJ5wQ9mht0iRswv3ccwr5hhYl6PcClmW8Lk8f25zLgJcyXjvwqpmVmlmPzZ1kZj3MLGVmqaqqqghliUi+qKqCK66AI48MC4498kjowydtE+5cFWV6ZbbHEjzrB806EYK+Y8bhDu5eYWbNgElm9o67T/vaF7oPJ7R8KC4uzvr9IpJfqqvh4Yfh5pthzRro0wduukkLj21rUUb05UCLjNfNgYpNP2RmrYHHgG7u/vHG4+5ekf67EhhDaAWJSMK9+iq0bg2/+Q0cfXRYxmDQIIV8HKIEfQnQ0sz2NbPGQHdgfOYHzGxv4AXgAndfnHF8RzPbaePPwEnA2/VVvIjkniVLwhIFP/tZGNGPHx8WIzvooLgrK1xbbN24+wYz6wW8AhQBI9x9vpn1TL8/DLgJ2A0YYmEBig3uXgzsAYxJH2sEPO3uLzfIlYhIrNasCfuy3nMPNGoUfr72Wth++7grE3PPvXZ4cXGxp1Kaci+SD9zhmWfguutg+XI4/3y48074wQ/irqywmFlpeoD9NXoyVkS22rx50KkTnHMONGsGr78OTz2lkM81CnoRqbX//hd694YjjghhP2xYWKumQ4e4K5NstHqliERWUwMjR4blCj7+OMyNv/VW2G23uCuTb6IRvYhEkkrBMceEtWgOOCC8HjJEIZ8PFPQi8o0+/hh69gyrSy5dCk8+CdOnh7aN5AcFvYhkVVMDjz4KBx4Ylg7u3RsWLYILLtA2fvlGPXoR+ZqSErjqqvD3cceFZQwOOyzuqmRraUQvIv9v5crQpjnqKFi2DP70p7DCpEI+vynoRYSaGhgx4uttmvPOU5smCdS6ESlws2fDlVfCG2+EefBDhoTFyCQ5NKIXKVCrVoWRe9u2Ya/Wxx+HadMU8kmkEb1IgXEPe7T+5jfw4YehJz9gAOy6a9yVSUNR0IsUkEWLwmyayZPDSP6vf4XirMtgSZKodSNSANatgxtvDG2ZVAoGD4a33lLIFwqN6EUSbuJE6NUL3nsvPOx0992wxx5xVyXbkkb0Igm1fDmceSb8/Odh848pU8LyBQr5wqOgF0mYDRvggQfC1n0TJoQbrXPmwAknxF2ZxEWtG5EEKSkJs2j+8Q/o0iX04vfbL+6qJG4a0YskwKpVcPXVYemCDz6A0aNDb14hL6ARvUhec4fnn4drroEVK8LUyQEDYOed465McolG9CJ5aulSOPXUcMN1zz3DdMmHHlLIy9cp6EXyTHV1mCJ5yCFhZclBg2DWLPjxj+OuTHKVWjcieWTWLOjRI8yiOfXUsE783nvHXZXkOo3oRfLAJ5+EPnz79lBVFfry48Yp5CUajehFcty4ceHJ1uXLw3LCAwbALrvEXZXkE43oRXLU8uVwxhlw2mnQpAnMnBlaNQp5qS0FvUiOqamBoUOhVaswF/6OO8IDUO3bx12Z5Cu1bkRyyIIF4WbrjBnwk5/AsGGw//5xVyX5TiN6kRzw+edw883Qpg0sXBh2e5o0SSEv9UMjepGYzZwJl18eAv6cc+D++6FZs7irkiSJNKI3sy5mtsjMysysX5b3zzOzuek/M83s8KjnihSq1avDbJqOHWHNmtCPf/pphbzUvy0GvZkVAYOBrkAr4Bwza7XJx94Djnf31sCtwPBanCtScF58MTzZOmRImB8/fz507Rp3VZJUUUb07YAyd1/i7uuBUUC3zA+4+0x3/0/65ZtA86jnihSSysrQnjnllDBl8o03Qqvmu9+NuzJJsihBvxewLON1efrY5lwGvFTbc82sh5mlzCxVVVUVoSyR/OEOTz0FBx8ML7wAt9wCpaVhWWGRhhYl6C3LMc/6QbNOhKD/XW3Pdffh7l7s7sVNmzaNUJZIfnj//dCWufDCsOvTP/8ZNupu3DjuyqRQRAn6cqBFxuvmQMWmHzKz1sBjQDd3/7g254okUU1N2OHp0EPh9dfhwQdh+vTwIJTIthQl6EuAlma2r5k1BroD4zM/YGZ7Ay8AF7j74tqcK5JEixbB8ceHWTUdOoSbrVdfDd/SkysSgy3Oo3f3DWbWC3gFKAJGuPt8M+uZfn8YcBOwGzDEzAA2pNswWc9toGsRiV11Ndx7L/TvD9/5DowcGVo2lq2JKbKNmHvWlnmsiouLPZVKxV2GSK3Mng2XXhp68GecERYg23PPuKuSQmFmpe5enO09/YekSB19/jncdFPY4Wn5cnjuufBHIS+5QksgiNTBrFlhFD9/PlxwAdx3H+y2W9xViXyVRvQiW2HdOujbF44+GlatCk+6PvmkQl5yk0b0IrU0Y0YYxS9eHJYUvusubQYiuU0jepGI1qyBPn3g2GNh/Xr429/gkUcU8pL7NKIXiWDqVLjsMvjXv+Cqq2DgQK1PI/lDI3qRb/Dpp+GhpxNOCOvVTJkSpk0q5CWfKOhFNmPKFGjd+sulhOfODYEvkm8U9CKb+PTT0J7p3BmKikLb5oEHYMcd465MZOso6EUyvPZaGMUPHQq9e8OcOeHmq0g+U9CL8GUvvlOnsPDY1KlhQ5DvfCfuykTqTkEvBW/aNDj88NCL79079OI1ipckUdBLwVq7NsyLP/74sLrka69pFC/JpHn0UpBmzoSLL4Z33w0tm4EDdbNVkksjeikon30G118PHTuGteP//nd46CGFvCSbRvRSMGbNCqP4hQvhiivg7rthp53irkqk4WlEL4m3fj387//CMcfAJ5/AK6/AsGEKeSkcGtFLos2ZE7bymzsXLrkkrBevRcik0GhEL4m0YQPcdlvY9enDD2H8eBgxQiEvhUkjekmchQvhoougpAS6dw+LkGlDEClkGtFLYtTUwKBBcMQRsGQJPPMM/OUvCnkRjeglEZYsCT34adPg1FNh+HBtzi2ykUb0ktfcQ6i3bg2zZ8Pjj8O4cQp5kUwa0UveqqiAyy+Hl16Cn/wk3Gzde++4qxLJPRrRS95xD733Qw8N69M8/DC8+qpCXmRzFPSSVz76CM4+G849Fw46KMyTv+qqsLSwiGSnfx6SNyZMCKP4sWPhjjtg+nRo2TLuqkRyn3r0kvNWr4Zrrw09+NatQ5umdeu4qxLJHxrRS07buEH3yJFwww3hISiFvEjtKOglJ61bFzYF6dwZGjeGGTNgwIDws4jUTqSgN7MuZrbIzMrMrF+W9w8yszfM7HMzu26T95aa2Twzm21mqfoqXJJr1qzwdOsDD8DVV4f58e3bx12VSP7aYo/ezIqAwcCJQDlQYmbj3X1BxsdWAtcAp23mazq5+0d1rFUSbv36sBDZ7bfD978Pf/tbmB8vInUTZUTfDihz9yXuvh4YBXTL/IC7V7p7CVDdADVKAXj77TBqv/VWOO88mDdPIS9SX6IE/V7AsozX5eljUTnwqpmVmlmPzX3IzHqYWcrMUlVVVbX4eslnX3wRdnpq2xbKy+GFF+CJJ6BJk7grE0mOKNMrLcsxr8Xv6ODuFWbWDJhkZu+4+7SvfaH7cGA4QHFxcW2+X/LUv/4VtvZ7/XU4/fSw61OzZnFXJZI8UUb05UCLjNfNgYqov8DdK9J/VwJjCK0gKWDuMHRomCY5bx489RQ8/7xCXqShRAn6EqClme1rZo2B7sD4KF9uZjua2U4bfwZOAt7e2mIl/5WXQ5cucOWV0LFj6M2ffz5Ytv9uFJF6scXWjbtvMLNewCtAETDC3eebWc/0+8PMbE8gBewM1JhZH6AVsDswxsK/4kbA0+7+coNcieQ0d3jySejdG6qrw4j+iisU8CLbQqQlENx9IjBxk2PDMn5eQWjpbGo1cHhdCpT8t2IF9OgBf/0rHHtsWDP+Rz+KuyqRwqEnY6VBPfMMHHJIWJ9m0KCwrLBCXmTbUtBLg9i4nHD37rD//uHp1muv1XLCInHQPzupdy+8EEbxY8aE9WlmzAhrx4tIPLRMsdSbjz+GXr1g1KiwVs2kSVppUiQXaEQv9WLs2DCKf+45uPlmeOsthbxIrtCIXupk5Uq45hr485+hTRt45RU4XPOsRHKKRvSy1caMgVatwsya/v3D8sIKeZHcoxG91FplZVgnfvTo0It/+eUwmheR3KQRvUTmDn/5SxjFjx0bZtS89ZZCXiTXaUQvkVRUwK9/DePHh3Xj//jHEPgikvs0opdvVFMDjz4aQn3SpPB06+uvK+RF8olG9LJZixeHNWqmToVOnWD48PCUq4jkF43o5Wuqq8O+ra1bw5w5oU0zebJCXiRfaUQvX1FSApdfDnPnwplnwoMPwp57xl2ViNSFRvQCwOrV4cGn9u3DUgZjx4bpkwp5kfynoC9w7mHZgoMPhocfDjNr5s+Hbt3irkxE6ouCvoAtXQqnnBJaNM2awZtvhrDfZZe4KxOR+qSgL0DV1XDnnWGK5NSpYcpkSQm007btIomkm7EFZsqUsJTwggVw2mnhZmuLFnFXJSINSSP6AlFRAeeeC507w9q14QnXMWMU8iKFQEGfcNXVcO+9cOCBYeenP/whjOZPPTXuykRkW1HrJsEy2zQnnxzaNNqYW6TwaESfQEuXwi9/+WWbZtw4mDBBIS9SqBT0CbJmDdx0U5gT/9JLcOutYTT/i1+AWdzViUhc1LpJAPewy9P110N5OZxzTpg+qRutIgIa0ee90lI47rgQ7k2bwvTp8PTTCnkR+ZKCPk9VVMDFF0NxMSxaFJYQLimBjh3jrkxEco1aN3lm7Vq4557QmtmwAfr2hRtu0LIFIrJ5Cvo8UVMT9mvt1y/04X/5yxD2++0Xd2UikuvUuskDr78elg8+/3zYYw+YNg2efVYhLyLRRAp6M+tiZovMrMzM+mV5/yAze8PMPjez62pzrmzekiVhZcljjw09+SeegFmzwmsRkai2GPRmVgQMBroCrYBzzGzTraFXAtcA92zFubKJ//43TJU8+GCYOBFuvjnccL3wQviW/htMRGopSmy0A8rcfYm7rwdGAV/ZlsLdK929BKiu7bnypQ0bYMiQsDfrvffCeefBu++Gh6B23DHu6kQkX0UJ+r2AZRmvy9PHooh8rpn1MLOUmaWqqqoifn1yTJoEbdrAVVfBYYeF+fEjRsAPfhB3ZSKS76IEfbaH5z3i90c+192Hu3uxuxc3bdo04tfnv8WLwxIFJ50E69aFFSb//nc44oi4KxORpIgS9OVA5nOWzYGKiN9fl3MTbdUq+O1v4dBD4bXXwlTJBQvg9NO1Lo2I1K8oQV8CtDSzfc2sMdAdGB/x++tybiK5w1NPhfXh77sPLroojOr79oXtt4+7OhFJoi0+MOXuG8ysF/AKUASMcPf5ZtYz/f4wM9sTSAE7AzVm1gdo5e6rs53bQNeS8+bNCz346dPhqKPgxRehbdu4qxKRpDP3qO32bae4uNhTqVTcZdSb1auhf/+w8UeTJjBwIFx6qaZKikj9MbNSdy/O9p6WQGhA7uEJ1j59YMUK+NWv4PbbYbfd4q5MRAqJgr6BrFgBV14ZNuBu2xbGjoV27eKuSkQKkZoH9WzjzdZWrcJTrXfeCW++qZAXkfhoRF+Pli+Hnj3D/qzHHBMeeDrwwLirEpFCpxF9PXCHkSPhkENg8uQwbXLaNIW8iOQGjejr6JNPwij+6afDln5//GNYq0ZEJFdoRF8Hs2eHG62jRsFtt4WlCxTyIpJrFPRbwR2GDg2bgaxZA1OmwP/8DxQVxV2ZiMjXKehradUqOPvsMHWyU6cwqj/uuLirEhHZPPXoa2HBAjj1VHj//TBt8rrr9HSriOQ+BX1EkyfDGWfAt78NU6dChw5xVyQiEo3GoxGMGAFdukDz5vDWWwp5EckvCvpvUFMTbrJedlnox8+YAT/8YdxViYjUjlo3m/HZZ3DJJWHq5K9+BYMHw3bbxV2ViEjtKeizWLky3HSdOTPcdL3+eu36JCL5S0G/iZUr4ac/DTNsRo+GM8+MuyIRkbpR0GfIDPmxY8MNWBGRfKebsWkKeRFJKgU9CnkRSbaCb91sDPn582HcOIW8iCRPQY/oM0NeI3kRSaqCDfrPPgtTKDeGfNeucVckItIwCrJ1U1MDF18c5smPHq2QF5FkK8gR/Y03wjPPwMCBmicvIslXcEE/YgTcfjtcfjn07Rt3NSIiDa+ggn7yZLjiCjjxRBgyRMsaiEhhKJigX7AgrCd/4IHw7LNaoExECkdBBP2HH8LJJ8MOO8CLL8Iuu8RdkYjItpP4WTfV1XD66VBZCdOmaT15ESk8iQ/6fv3gjTfCuvLFxXFXIyKy7UVq3ZhZFzNbZGZlZtYvy/tmZg+m359rZkdmvLfUzOaZ2WwzS9Vn8VsybhwMGgRXXglnn70tf7OISO7Y4ojezIqAwcCJQDlQYmbj3X1Bxse6Ai3Tf44Chqb/3qiTu39Ub1VH8N574aGotm1D2IuIFKooI/p2QJm7L3H39cAooNsmn+kGPOnBm0ATM/t+Pdca2eefw1lngXt48nX77eOqREQkflGCfi9gWcbr8vSxqJ9x4FUzKzWzHpv7JWbWw8xSZpaqqqqKUNbmXX89pFLw+OOw3351+ioRkbwXJeizPVbktfhMB3c/ktDeucrMjsv2S9x9uLsXu3tx06ZNI5SV3XPPwUMPQZ8+YbaNiEihixL05UCLjNfNgYqon3H3jX9XAmMIraAGUVYGl14K7dqFTb1FRCRa0JcALc1sXzNrDHQHxm/ymfHAhenZN+2BVe7+gZntaGY7AZjZjsBJwNv1WP//++yzsEBZo0ahL9+4cUP8FhGR/LPFWTfuvsHMegGvAEXACHefb2Y90+8PAyYCJwNlwFrgkvTpewBjLCwq0wh42t1frverAL74Ag47DG65RQ9FiYhkMvdN2+3xKy4u9lRqm065FxHJa2ZW6u5ZHwstiLVuREQKmYJeRCThFPQiIgmnoBcRSTgFvYhIwinoRUQSTkEvIpJwCnoRkYTLyQemzKwKeH8rT98d2KZr3+cIXXdh0XUXlijX/UN3z7oiZE4GfV2YWWpzT4clma67sOi6C0tdr1utGxGRhFPQi4gkXBKDfnjcBcRE111YdN2FpU7XnbgevYiIfFUSR/QiIpJBQS8iknCJCXoz62Jmi8yszMz6xV1PQzKzEWZWaWZvZxz7nplNMrN303/vGmeN9c3MWpjZFDNbaGbzzax3+njSr/vbZjbLzOakr/vm9PFEX/dGZlZkZv80swnp14Vy3UvNbJ6ZzTazVPrYVl97IoLezIqAwUBXoBVwjpm1ireqBjUS6LLJsX7AZHdvCUxOv06SDcBv3f1goD1wVfp/46Rf9+dAZ3c/HGgDdEnvy5z0696oN7Aw43WhXDdAJ3dvkzF/fquvPRFBD7QDytx9ibuvB0YB3WKuqcG4+zRg5SaHuwFPpH9+AjhtW9bU0Nz9A3f/R/rnTwj/+Pci+dft7v5p+uV26T9Owq8bwMyaAz8HHss4nPjr/gZbfe1JCfq9gGUZr8vTxwrJHu7+AYRQBJrFXE+DMbN9gCOAtyiA6063L2YDlcAkdy+I6wbuB/oCNRnHCuG6Ifyf+atmVmpmPdLHtvraGzVAgXGwLMc0bzSBzOy7wPNAH3dfbZbtf/pkcfcvgDZm1gQYY2aHxlxSgzOzU4BKdy81sxNiLicOHdy9wsyaAZPM7J26fFlSRvTlQIuM182BiphqicuHZvZ9gPTflTHXU+/MbDtCyP/Z3V9IH078dW/k7v8FXiPcn0n6dXcAfmFmSwmt2M5m9ieSf90AuHtF+u9KYAyhPb3V156UoC8BWprZvmbWGOgOjI+5pm1tPHBR+ueLgHEx1lLvLAzd/wgsdPdBGW8l/bqbpkfymNkOwE+Bd0j4dbv77929ubvvQ/j3/Hd3P5+EXzeAme1oZjtt/Bk4CXibOlx7Yp6MNbOTCT29ImCEuw+It6KGY2Z/AU4gLF36IfAHYCwwGtgb+DdwprtvesM2b5lZR2A6MI8ve7Y3EPr0Sb7u1oQbb0WEgdlod7/FzHYjwdedKd26uc7dTymE6zaz/QijeAjt9afdfUBdrj0xQS8iItklpXUjIiKboaAXEUk4Bb2ISMIp6EVEEk5BLyKScAp6kQjMrImZXRl3HSJbQ0EvEk0TQEEveUlBLxLNQOBH6fXB7467GJHa0ANTIhGkV8yc4O6JX1BMkkcjehGRhFPQi4gknIJeJJpPgJ3iLkJkayjoRSJw94+BGWb2tm7GSr7RzVgRkYTTiF5EJOEU9CIiCaegFxFJOAW9iEjCKehFRBJOQS8iknAKehGRhPs/crvI7m2IixIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(1)\n",
    "axs.set_xlabel(\"t\")\n",
    "'train_loss_record1'\n",
    "axs.plot(train_loss_record1[:50],\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 77ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = NN.predict(train_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for _ in range(epochs):\n",
    "    with tf.GradientTape(persistent=True) as tape: \n",
    "        loss = shared_loss(train_t,NN,T,g)\n",
    "        \n",
    "    grad1 = tape.gradient(loss, trainable_vars1)\n",
    "    grad2 = tape.gradient(loss, trainable_vars2)\n",
    "    \n",
    "    hess_xy = tape.gradient(grad1,trainable_vars2)\n",
    "    hess_yx = tape.gradient(grad2,trainable_vars1)\n",
    "    \n",
    "    \n",
    "    optimizer.apply_gradients(grad,hess_xy, trainable_vars)\n",
    "    train_loss_record1.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "for k in var.shape:\n",
    "    print(k== None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = NN.trainable_variables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "print(tf.squeeze(var1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for var in trainable_vars1:\n",
    "    print(np.max(var.shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beispiel docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape(persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m t2: \n\u001b[0;32m     16\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape(persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m t1: \n\u001b[1;32m---> 17\u001b[0m       loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mshared_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m   g \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m.\u001b[39mgradient(loss1, trainable_vars1)\n\u001b[0;32m     20\u001b[0m h \u001b[38;5;241m=\u001b[39m t2\u001b[38;5;241m.\u001b[39mjacobian(g, trainable_vars2)\n",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36mshared_loss\u001b[1;34m(t, NN, T, g)\u001b[0m\n\u001b[0;32m     10\u001b[0m     dnn \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(u, t)\n\u001b[0;32m     11\u001b[0m     ddnn \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(dnn,t)\n\u001b[1;32m---> 12\u001b[0m ode_loss \u001b[38;5;241m=\u001b[39m  (a \u001b[38;5;241m*\u001b[39m ddnn \u001b[38;5;241m+\u001b[39m b \u001b[38;5;241m*\u001b[39m dnn \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m*\u001b[39m u \u001b[38;5;241m-\u001b[39m \u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     14\u001b[0m iv_loss \u001b[38;5;241m=\u001b[39m NN(t_0) \u001b[38;5;66;03m# we preceed with one intital value loss y_prime\u001b[39;00m\n\u001b[0;32m     15\u001b[0m square_loss \u001b[38;5;241m=\u001b[39m T(t) \u001b[38;5;241m*\u001b[39m( tf\u001b[38;5;241m.\u001b[39msquare(ode_loss) \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39msquare(iv_loss) )\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object is not callable"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal([7, 5])\n",
    "l1 = tf.keras.layers.Dense(8, activation=tf.nn.relu)\n",
    "l2 = tf.keras.layers.Dense(6, activation=tf.nn.relu)\n",
    "\n",
    "with tf.GradientTape() as t2:\n",
    "  with tf.GradientTape() as t1:\n",
    "    x = l1 (x)\n",
    "    x = l2 (x)\n",
    "    loss = tf.reduce_mean(x **2)\n",
    "\n",
    "  g = t1.gradient(loss, l1.kernel)\n",
    "\n",
    "h = t2.jacobian(g, l1.kernel)\n",
    "NN.trainable_variables[0]\n",
    "with tf.GradientTape(persistent=True) as t2: \n",
    "  with tf.GradientTape(persistent=True) as t1: \n",
    "      loss1 = shared_loss(train_t,NN,T,g)\n",
    "  g = t1.gradient(loss1, trainable_vars1)\n",
    "\n",
    "h = t2.jacobian(g, trainable_vars2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=40>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_params = tf.reduce_prod(layer1.kernel.shape)\n",
    "n_params\n",
    "# g_vec = tf.reshape(g, [n_params, 1])\n",
    "# h_mat = tf.reshape(h, [n_params, n_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.linalg.inv(  Dyx[0] @ Dxy[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig, axs = plt.subplots(1)\n",
    "# axs.set_xlabel(\"t\")\n",
    "# axs.set_ylabel(\"pred\")\n",
    "# axs.plot(train_loss_record1,\"b\")\n",
    "# axs.spines['top'].set_visible(False)\n",
    "# axs.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = shared_loss(train_t,NN,T,g)\n",
    "grad2 = tape.gradient(loss, trainable_vars2)\n",
    "# hess_yx = tape.gradient(grad2,trainable_vars1)\n",
    "grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C_GradientDescent(tf.Module):\n",
    "\n",
    "  def __init__(self, learning_rate=1e-3):\n",
    "    # Initialize parameters\n",
    "    self.learning_rate = learning_rate\n",
    "    self.title = f\"Gradient descent optimizer: learning rate={self.learning_rate}\"\n",
    "\n",
    "  def apply_gradients(self, grads,hess, vars):\n",
    "    # Update variables\n",
    "    for grad,hess, var in zip(grads,hess, vars):\n",
    "      print(type(var))\n",
    "      print(type(hess))\n",
    "      print(type(grads))\n",
    "      var.assign_sub(self.learning_rate*grad + self.learning_rate**2  * (hess * grad))\n",
    "# dy_dx = gg.gradient(y, x)\n",
    "loss = shared_loss(train_t,NN,T,g)\n",
    "grad2 = tape.gradient(loss, trainable_vars2)\n",
    "hess_yx = tape.gradient(grad2,trainable_vars1)\n",
    "\n",
    "optimizer = C_GradientDescent()\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for _ in range(epochs):\n",
    "    with tf.GradientTape(persistent=True) as tape: \n",
    "        loss = shared_loss(train_t,NN,T,g)\n",
    "        \n",
    "    grad1 = tape.gradient(loss, trainable_vars1)\n",
    "    grad2 = tape.gradient(loss, trainable_vars2)\n",
    "    \n",
    "    hess_xy = tape.gradient(grad1,trainable_vars2)\n",
    "    hess_yx = tape.gradient(grad2,trainable_vars1)\n",
    "    \n",
    "    \n",
    "    optimizer.apply_gradients(grad,hess_xy, trainable_vars)\n",
    "    train_loss_record1.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(32):\n",
    "    for j in range(32):\n",
    "        \n",
    "        A = tf.zeros((32,32))\n",
    "        for k in range(0,32):\n",
    "            tf.add(Dxy[i][k] @ Dyx[k][j],A)\n",
    "        # B[i][j] = A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "links\n",
    "https://openreview.net/pdf?id=HJechEHeLr\n",
    "https://www.tensorflow.org/guide/advanced_autodiff#hessian\n",
    "https://proceedings.mlr.press/v119/schaefer20a/schaefer20a.pdf\n",
    "https://f-t-s.github.io/projects/cgd/\n",
    "https://arxiv.org/pdf/2204.11144.pdf\n",
    "\n",
    "latex nn\n",
    "http://chebe163.caltech.edu/2018w/handouts/intro_to_latex.html#:~:text=The%20align%20environment%20in%20Latex,line%20at%20the%20alignment%20point.\n",
    "https://deeplearningmath.org/general-fully-connected-neural-networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
