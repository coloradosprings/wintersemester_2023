{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GDA and CGD updates pytorch https://github.com/julienroyd/competitive_gradient_descent/blob/master/exp2_gaussian_mixture/CGD_vs_GDA_GaussianMixture_GAN.ipynb\n",
    "def compute_gda_update(f, x, g, y, eta=None):\n",
    "    \"\"\"\n",
    "    Computes the gradient step for both players\n",
    "    f: loss function to minimise for player X\n",
    "    x: current action (parameters) of player X\n",
    "    g: loss function to minimise for player Y\n",
    "    y: current action (parameters) of player y\n",
    "    \"\"\"\n",
    "    x_update = list(grad(outputs=f, inputs=x, retain_graph=True))\n",
    "    y_update = list(grad(outputs=g, inputs=y))\n",
    "\n",
    "    return x_update, y_update\n",
    "\n",
    "def compute_cgd_update(f, x, g, y, eta, max_it=10):\n",
    "    \"\"\"\n",
    "    Iteratively estimate the solution for the local Nash equilibrium using the conjugate gradient method\n",
    "    f: loss function to minimise for player X\n",
    "    x: current action (parameters) of player X\n",
    "    g: loss function to minimise for player Y\n",
    "    y: current action (parameters) of player y\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    # Computing the gradients\n",
    "\n",
    "    df_dx = grad(outputs=f, inputs=x, create_graph=True, retain_graph=True)\n",
    "    dg_dy = grad(outputs=g, inputs=y, create_graph=True, retain_graph=True)\n",
    "\n",
    "    df_dy = grad(outputs=f, inputs=y, create_graph=True, retain_graph=True)\n",
    "    dg_dx = grad(outputs=g, inputs=x, create_graph=True, retain_graph=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Creating the appropriate structure for the parameter updates and initialising to 0\n",
    "\n",
    "        x_update, y_update = [], []\n",
    "        for x_grad_group, y_grad_group in zip(df_dx, dg_dy):\n",
    "            x_update.append(torch.zeros_like(x_grad_group))\n",
    "            y_update.append(torch.zeros_like(y_grad_group))\n",
    "\n",
    "        # Creating the appropriate structure for the residuals and basis vectors and initialise them\n",
    "\n",
    "        r_xk, r_yk, p_xk, p_yk = [], [], [], []\n",
    "        for x_param_update, y_param_update in zip(df_dx, dg_dy):\n",
    "            r_xk.append(torch.clone(x_param_update))\n",
    "            p_xk.append(torch.clone(x_param_update))\n",
    "            r_yk.append(torch.clone(y_param_update))\n",
    "            p_yk.append(torch.clone(y_param_update))\n",
    "\n",
    "    # Iteratively solve for the local Nash Equilibrium\n",
    "\n",
    "    for k in range(max_it):\n",
    "\n",
    "        # Computes the Hessian-vector product Ap\n",
    "\n",
    "        hvp_x = grad(outputs=df_dy, inputs=x, grad_outputs=p_yk, retain_graph=True)\n",
    "        hvp_y = grad(outputs=dg_dx, inputs=y, grad_outputs=p_xk, retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Computes the matrix-basisVector product Ap\n",
    "\n",
    "            Ap_x, Ap_y = [], []\n",
    "            for i in range(len(p_xk)):\n",
    "                Ap_x.append(p_xk[i] + eta * hvp_x[i])\n",
    "                Ap_y.append(p_yk[i] + eta * hvp_y[i])\n",
    "\n",
    "            # Computes step size alpha_k\n",
    "\n",
    "            num, denom = 0., 0.\n",
    "            for i in range(len(r_xk)):\n",
    "\n",
    "                r_k_i = torch.cat([r_xk[i].flatten(), r_yk[i].flatten()])\n",
    "                num += torch.dot(r_k_i, r_k_i)\n",
    "\n",
    "                Ap_i = torch.cat([Ap_x[i].flatten(), Ap_y[i].flatten()])\n",
    "                p_k_i = torch.cat([p_xk[i].flatten(), p_yk[i].flatten()])\n",
    "\n",
    "                denom += torch.dot(p_k_i, Ap_i)\n",
    "\n",
    "            alpha_k = num / denom\n",
    "\n",
    "            # Computes new updates\n",
    "\n",
    "            for i in range(len(x_update)):\n",
    "                x_update[i] += alpha_k * p_xk[i]\n",
    "                y_update[i] += alpha_k * p_yk[i]\n",
    "\n",
    "            # Computes new residuals\n",
    "\n",
    "            r_xkplus1, r_ykplus1 = [], []\n",
    "            for i in range(len(r_xk)):\n",
    "                r_xkplus1.append(r_xk[i] - alpha_k * Ap_x[i])\n",
    "                r_ykplus1.append(r_yk[i] - alpha_k * Ap_y[i])\n",
    "\n",
    "            # Check convergence condition\n",
    "\n",
    "            r_xkplus1_squared_sum, r_ykplus1_squared_sum = 0., 0.\n",
    "            x_update_squared_norm, y_update_squared_norm = 0., 0.\n",
    "            for i in range(len(r_xkplus1)):\n",
    "                r_xkplus1_squared_sum += torch.sum(r_xkplus1[i] ** 2.)\n",
    "                r_ykplus1_squared_sum += torch.sum(r_ykplus1[i] ** 2.)\n",
    "\n",
    "                x_update_squared_norm += torch.sum(x_update[i] ** 2.)\n",
    "                y_update_squared_norm += torch.sum(y_update[i] ** 2.)\n",
    "\n",
    "            r_kplus1_norm = torch.sqrt(r_xkplus1_squared_sum + r_ykplus1_squared_sum)\n",
    "            update_norm = torch.sqrt(x_update_squared_norm + y_update_squared_norm)\n",
    "\n",
    "            if r_kplus1_norm <= 1e-6:\n",
    "                break\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Computes beta_k\n",
    "\n",
    "                num, denom = 0., 0.\n",
    "                for i in range(len(r_xk)):\n",
    "                    r_kplus1_i = torch.cat([r_xkplus1[i].flatten(), r_ykplus1[i].flatten()])\n",
    "                    denom += torch.dot(r_kplus1_i, r_kplus1_i)\n",
    "\n",
    "                    r_k_i = torch.cat([r_xk[i].flatten(), r_yk[i].flatten()])\n",
    "                    denom += torch.dot(r_k_i, r_k_i)\n",
    "\n",
    "                beta_k = num / denom\n",
    "\n",
    "                # Computes new basis vectors\n",
    "\n",
    "                for i in range(len(p_xk)):\n",
    "                    p_xk[i] = r_xkplus1[i] + beta_k * p_xk[i]\n",
    "                    p_yk[i] = r_ykplus1[i] + beta_k * p_yk[i]\n",
    "\n",
    "                r_xk = deepcopy(r_xkplus1)\n",
    "                r_yk = deepcopy(r_ykplus1)\n",
    "\n",
    "    return x_update, y_update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
